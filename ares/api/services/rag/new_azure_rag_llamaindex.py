# ares/api/services/rag/new_azure_rag_llamaindex.py
import os
import io
import json
import hashlib
import tempfile
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from datetime import datetime, timezone

from tqdm import tqdm
from azure.core.credentials import AzureKeyCredential
from azure.storage.blob import BlobServiceClient, BlobClient
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient

from llama_index.core import VectorStoreIndex, Settings, StorageContext, Document
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.azure_openai import AzureOpenAI
from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding
from llama_index.readers.file import PyMuPDFReader
from llama_index.vector_stores.azureaisearch import AzureAISearchVectorStore, IndexManagement


# ============================== ë©”íƒ€ ì €ì¥ì†Œ ==============================
class _MetaStore:
    """
    ê°„ë‹¨í•œ íŒŒì¼ ê¸°ë°˜ ë©”íƒ€ ì €ì¥ì†Œ.
    êµ¬ì¡°:
    {
      "<blob_name>": {
        "etag": "...",
        "sha256": "...",
        "last_modified": "2024-09-01T09:00:00+00:00"
      },
      ...
    }
    """
    def __init__(self, path: Optional[str] = None):
        default_path = ".rag_meta.json"
        self.path = path or os.getenv("AZURE_RAG_META_PATH", default_path)
        self._data: Dict[str, Dict[str, str]] = {}
        self._load()

    def _load(self):
        try:
            if os.path.exists(self.path):
                with open(self.path, "r", encoding="utf-8") as f:
                    self._data = json.load(f)
            else:
                self._data = {}
        except Exception as e:
            print(f"âš ï¸ ë©”íƒ€ ì €ì¥ì†Œ ë¡œë“œ ì‹¤íŒ¨({self.path}): {e} â†’ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            self._data = {}

    def save(self):
        try:
            with open(self.path, "w", encoding="utf-8") as f:
                json.dump(self._data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ë©”íƒ€ ì €ì¥ì†Œ ì €ì¥ ì‹¤íŒ¨({self.path}): {e}")

    def get(self, key: str) -> Optional[Dict[str, str]]:
        return self._data.get(key)

    def set(self, key: str, value: Dict[str, str]):
        self._data[key] = value

    def delete(self, key: str):
        if key in self._data:
            del self._data[key]

    def keys(self):
        return list(self._data.keys())


# ============================== ìœ í‹¸ ==============================
def _sanitize_id(name: str) -> str:
    """íŒŒì¼ëª…ì„ ì¸ë±ìŠ¤ doc_idë¡œ ì‚¬ìš©í•  ë•Œ ì•ˆì „í•˜ê²Œ ë³€í™˜"""
    return name.replace("[", "_").replace("]", "_")


def _iso(dt: datetime) -> str:
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    return dt.astimezone(timezone.utc).isoformat()


def _calc_sha256_streaming(stream: io.BufferedReader, chunk_size: int = 1024 * 1024) -> str:
    """ëŒ€ìš©ëŸ‰ íŒŒì¼ì— ëŒ€í•´ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ sha256 ê³„ì‚°"""
    h = hashlib.sha256()
    while True:
        chunk = stream.read(chunk_size)
        if not chunk:
            break
        h.update(chunk)
    return h.hexdigest()


# ============================== ë³¸ì²´ ==============================
class AzureBlobRAGSystem:
    """Azure Blob + Azure AI Search ê¸°ë°˜ RAG ì‹œìŠ¤í…œ (ì¦ë¶„ ì¸ë±ì‹±: ETag/sha256/ë©”íƒ€ ì €ì¥)"""

    def __init__(self, container_name: str, index_name: str):
        print(f"ğŸš€ Azure í†µí•© RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì»¨í…Œì´ë„ˆ: {container_name}, ì¸ë±ìŠ¤: {index_name})...")
        self.container_name = container_name
        self.index_name = index_name
        self.query_engine = None

        self._require_env([
            "AZURE_OPENAI_ENDPOINT",
            "AZURE_OPENAI_KEY",
            "AZURE_STORAGE_CONNECTION_STRING",
            "AZURE_SEARCH_ENDPOINT",
            "AZURE_SEARCH_KEY",
        ])

        # Azure í´ë¼ì´ì–¸íŠ¸
        connect_str = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
        search_endpoint = os.getenv("AZURE_SEARCH_ENDPOINT")
        search_key = os.getenv("AZURE_SEARCH_KEY")
        credential = AzureKeyCredential(search_key)

        self.blob_service_client = BlobServiceClient.from_connection_string(connect_str)
        self.container_client = self.blob_service_client.get_container_client(self.container_name)

        self.search_client = SearchClient(endpoint=search_endpoint, index_name=self.index_name, credential=credential)
        self.search_index_client = SearchIndexClient(endpoint=search_endpoint, credential=credential)

        # LlamaIndex ì„¤ì •
        self._setup_llamaindex()

        # ë²¡í„° ìŠ¤í† ì–´/ì¸ë±ìŠ¤/ì¿¼ë¦¬ì—”ì§„
        try:
            self.vector_store = AzureAISearchVectorStore(
                search_or_index_client=self.search_index_client,
                index_name=self.index_name,
                id_field_key="id",
                chunk_field_key="chunk",
                embedding_field_key="embedding",
                metadata_string_field_key="metadata",
                doc_id_field_key="doc_id",
                index_management=IndexManagement.CREATE_IF_NOT_EXISTS,
            )
            self.storage_context = StorageContext.from_defaults(vector_store=self.vector_store)
            self.index = VectorStoreIndex.from_vector_store(self.vector_store)
            self.query_engine = self.index.as_query_engine(similarity_top_k=5)
            print("âœ… Azure AI Search VectorStore ë° ì¿¼ë¦¬ ì—”ì§„ ì„¤ì • ì™„ë£Œ")
        except Exception as e:
            raise ConnectionError(f"Azure AI Search VectorStore ì„¤ì • ì‹¤íŒ¨: {e}")

        # ë©”íƒ€ ì €ì¥ì†Œ
        self.meta_store = _MetaStore()

    def is_ready(self) -> bool:
        """RAG ì‹œìŠ¤í…œì´ ì¿¼ë¦¬ë¥¼ ìˆ˜í–‰í•  ì¤€ë¹„ê°€ ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        return self.query_engine is not None

    # -------------------------- ë‚´ë¶€ ì„¤ì • --------------------------
    def _require_env(self, keys: List[str]):
        missing = [k for k in keys if not os.getenv(k)]
        if missing:
            raise ValueError(f"í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ ëˆ„ë½: {', '.join(missing)}")

    def _setup_llamaindex(self):
        print("ğŸ”§ LlamaIndex êµ¬ì„± ìš”ì†Œ ì„¤ì • ì¤‘...")
        Settings.llm = AzureOpenAI(
            engine=os.getenv("AZURE_OPENAI_MODEL"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            api_key=os.getenv("AZURE_OPENAI_KEY"),
            api_version=os.getenv("API_VERSION", "2024-02-15-preview"),
        )
        Settings.embed_model = AzureOpenAIEmbedding(
            model=os.getenv("AZURE_EMBEDDING_MODEL", "text-embedding-3-small"),
            deployment_name=os.getenv("AZURE_EMBEDDING_MODEL", "text-embedding-3-small"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            api_key=os.getenv("AZURE_OPENAI_KEY"),
            api_version="2023-05-15",
        )
        Settings.node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=200)
        print("  âœ… LlamaIndex ì„¤ì • ì™„ë£Œ")

    # -------------------------- ì¸ë±ìŠ¤ ë©”íƒ€ ì¡°íšŒ --------------------------
    def _get_indexed_doc_metadata(self) -> Dict[str, Dict[str, str]]:
        """
        AI Search ì¸ë±ìŠ¤ì— ì´ë¯¸ ë“¤ì–´ìˆëŠ” ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¨ë‹¤.
        ë°˜í™˜: { doc_id: {"last_modified": "...", "etag": "...", "sha256": "..."} }
        """
        print("ğŸ“Š AI Search ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì¤‘...")
        indexed: Dict[str, Dict[str, str]] = {}
        try:
            # metadataëŠ” stringified JSONìœ¼ë¡œ ì €ì¥ë˜ì–´ ìˆìŒ
            results = self.search_client.search(search_text="*", select=["doc_id", "metadata"])
            for doc in results:
                try:
                    meta = json.loads(doc.get("metadata", "{}"))
                    indexed[doc["doc_id"]] = {
                        "last_modified": meta.get("last_modified", ""),
                        "etag": meta.get("etag", ""),
                        "sha256": meta.get("sha256", ""),
                    }
                except Exception:
                    continue
            print(f"  âœ… ì¸ë±ìŠ¤ì— {len(indexed)}ê°œ ë¬¸ì„œ ë©”íƒ€ ìˆ˜ì§‘.")
        except Exception as e:
            print(f"  âš ï¸ ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨(ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŒ): {e}")
        return indexed

    # -------------------------- Blob â†’ Document ë¡œë”© --------------------------
    def _download_to_temp_and_hash(self, blob_client: BlobClient) -> Tuple[str, str]:
        """
        Blobì„ ì„ì‹œíŒŒì¼ë¡œ ì €ì¥í•˜ê³  sha256ì„ ê³„ì‚°í•œë‹¤.
        ë°˜í™˜: (temp_filepath, sha256_hex)
        """
        props = blob_client.get_blob_properties()
        total = props.size or None

        # ì„ì‹œíŒŒì¼ ìƒì„±
        suffix = Path(blob_client.blob_name).suffix
        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
        tmp_path = tmp.name

        sha256 = hashlib.sha256()
        try:
            stream = blob_client.download_blob()
            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì½ì–´ ì„ì‹œíŒŒì¼ì— ì“°ê³  ë™ì‹œì— í•´ì‹œ ê³„ì‚°
            for chunk in stream.chunks():
                if not isinstance(chunk, (bytes, bytearray)):
                    chunk = chunk.readall()
                tmp.write(chunk)
                sha256.update(chunk)
            tmp.close()
            return tmp_path, sha256.hexdigest()
        except Exception as e:
            try:
                tmp.close()
            except Exception:
                pass
            try:
                if os.path.exists(tmp_path):
                    os.remove(tmp_path)
            except Exception:
                pass
            raise RuntimeError(f"Blob ë‹¤ìš´ë¡œë“œ/í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {blob_client.blob_name}, {e}")

    def load_documents_from_blob(self, blobs_to_process: List[BlobClient]) -> List[Document]:
        """
        ì§€ì •ëœ Blob ëª©ë¡ì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ì—¬ LlamaIndex Document ê°ì²´ë¡œ ë³€í™˜.
        ê° Document.metadataì— file_name/container/source/last_modified/etag/sha256 í¬í•¨.
        """
        if not blobs_to_process:
            return []

        documents: List[Document] = []
        pdf_reader = PyMuPDFReader()
        print("ğŸ“– ì§€ì •ëœ Blobì—ì„œ ë¬¸ì„œ ë¡œë”© ì‹œì‘...")

        for blob_client in blobs_to_process:
            blob_name = blob_client.blob_name
            print(f"  ğŸ” ì²˜ë¦¬ ì¤‘: {blob_name}")

            try:
                props = blob_client.get_blob_properties()
                etag = getattr(props, "etag", "") or ""
                last_modified = props.last_modified.replace(tzinfo=timezone.utc)

                # ë‹¤ìš´ë¡œë“œ + í•´ì‹œ
                temp_path, sha256_hex = self._download_to_temp_and_hash(blob_client)

                try:
                    # íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ë¡œë”©
                    if blob_name.lower().endswith(".pdf"):
                        loaded_docs = pdf_reader.load_data(file_path=temp_path)
                    elif blob_name.lower().endswith(".txt"):
                        text = Path(temp_path).read_text(encoding="utf-8", errors="ignore")
                        loaded_docs = [Document(text=text)]
                    else:
                        print("    â„¹ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” í™•ì¥ì. ìŠ¤í‚µ:", blob_name)
                        continue

                    # ë©”íƒ€ë°ì´í„° ë¶€ì—¬
                    for doc in loaded_docs:
                        doc.id_ = _sanitize_id(blob_name)
                        doc.metadata.update({
                            "file_name": _sanitize_id(blob_name),
                            "container": self.container_name,
                            "source": "azure_blob",
                            "last_modified": _iso(last_modified),
                            "etag": etag,
                            "sha256": sha256_hex,
                        })

                    documents.extend(loaded_docs)
                    print(f"    âœ… '{blob_name}' ë¡œë“œ ì™„ë£Œ ({len(loaded_docs)}ê°œ ë¬¸ì„œ)")

                finally:
                    # ì„ì‹œíŒŒì¼ ì œê±°
                    try:
                        os.remove(temp_path)
                    except Exception:
                        pass

            except Exception as e:
                print(f"    âŒ '{blob_name}' ë¡œë“œ ì‹¤íŒ¨: {e}")

        print(f"âœ… ì´ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ.")
        return documents

    # -------------------------- ì‚­ì œ --------------------------
    def delete_doc(self, doc_id: str):
        """AI Searchì—ì„œ íŠ¹ì • ë¬¸ì„œ IDì™€ ê´€ë ¨ëœ ëª¨ë“  ì²­í¬ ì‚­ì œ"""
        print(f"ğŸ—‘ï¸ ì¸ë±ìŠ¤ì—ì„œ '{doc_id}' ë¬¸ì„œ ì‚­ì œ ì¤‘...")
        try:
            self.index.delete_ref_doc(ref_doc_id=doc_id, delete_from_docstore=True)
            print(f"  âœ… '{doc_id}' ì‚­ì œ ì™„ë£Œ.")
        except Exception as e:
            print(f"  âŒ '{doc_id}' ì‚­ì œ ì‹¤íŒ¨: {e}")

    # -------------------------- ë™ê¸°í™” --------------------------
    def sync_index(self, company_name_filter: Optional[str] = None):
        """
        Blob â†” ì¸ë±ìŠ¤ ì¦ë¶„ ë™ê¸°í™”.
        - ì¶”ê°€/ë³€ê²½ íŒë‹¨ ê¸°ì¤€: ETag + sha256 (ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ë³€ê²½ ì‹œ ì—…ë°ì´íŠ¸)
        - ë©”íƒ€ ì €ì¥ì†Œì™€ë„ ë™ê¸°í™” (ë¡œì»¬ JSON)
        - ì‚­ì œ: Blob/ë©”íƒ€/ì¸ë±ìŠ¤ ê°„ ë¶ˆì¼ì¹˜ ì •ë¦¬
        """
        print("\n" + "=" * 64)
        print("ğŸ”„ Azure AI Search ì¸ë±ìŠ¤ ì¦ë¶„ ë™ê¸°í™” ì‹œì‘...")

        # 0) ì†ŒìŠ¤ ë‚˜ì—´
        source_blobs: Dict[str, Dict[str, str]] = {}
        for blob in self.container_client.list_blobs():
            name = blob.name
            if not name.lower().endswith((".pdf", ".txt")):
                continue
            if company_name_filter:
                expected_prefix = f"[{company_name_filter}]"
                if not name.startswith(expected_prefix):
                    continue
            source_blobs[name] = {
                "last_modified": _iso(blob.last_modified.replace(tzinfo=timezone.utc)),
                "etag": getattr(blob, "etag", "") or "",
            }

        # 1) ì¸ë±ìŠ¤/ë©”íƒ€ ì €ì¥ì†Œ í˜„í™©
        indexed_docs = self._get_indexed_doc_metadata()            # {doc_id: {...}}
        meta_keys = set(self.meta_store.keys())                    # blob_name ì§‘í•©
        source_set = set(source_blobs.keys())
        indexed_set = set(indexed_docs.keys())

        # 2) ì‚­ì œ ëŒ€ìƒ
        to_delete_in_index = indexed_set - source_set              # ì¸ë±ìŠ¤ì—ëŠ” ìˆìœ¼ë‚˜ Blobì— ì—†ëŠ” ê²ƒ
        to_delete_in_meta = meta_keys - source_set                 # ë©”íƒ€ì—ëŠ” ìˆìœ¼ë‚˜ Blobì— ì—†ëŠ” ê²ƒ

        if to_delete_in_index:
            print(f"\nğŸ§¹ Blobì— ì—†ëŠ” {len(to_delete_in_index)}ê°œë¥¼ ì¸ë±ìŠ¤ì—ì„œ ì‚­ì œí•©ë‹ˆë‹¤.")
            for fname in to_delete_in_index:
                self.delete_doc(fname)

        if to_delete_in_meta:
            print(f"\nğŸ§¹ Blobì— ì—†ëŠ” {len(to_delete_in_meta)}ê°œë¥¼ ë©”íƒ€ ì €ì¥ì†Œì—ì„œ ì •ë¦¬í•©ë‹ˆë‹¤.")
            for fname in to_delete_in_meta:
                self.meta_store.delete(fname)

        # 3) ë³€ê²½/ì¶”ê°€ íŒì •
        to_process: List[str] = []
        for fname, src_meta in source_blobs.items():
            src_etag = src_meta.get("etag", "")
            meta_entry = self.meta_store.get(fname) or {}
            meta_etag = meta_entry.get("etag", "")
            meta_sha = meta_entry.get("sha256", "")

            # ì¸ë±ìŠ¤ ë©”íƒ€(ì°¸ê³ )
            idx_entry = indexed_docs.get(fname) or {}
            idx_etag = idx_entry.get("etag", "")
            idx_sha = idx_entry.get("sha256", "")

            # ìš°ì„ ìˆœìœ„: ETagê°€ ë‹¤ë¥´ë©´ ë‹¤ìš´ë¡œë“œ/í•´ì‹œ í›„ ë¹„êµ â†’ sha ë³€ê²½ ì—¬ë¶€ ìµœì¢…íŒì •
            if src_etag and src_etag == meta_etag and meta_sha and meta_sha == idx_sha:
                # ì†ŒìŠ¤ ETag = ë©”íƒ€ ETag = ì¸ë±ìŠ¤ sha ë™ì¼ â†’ ìŠ¤í‚µ
                continue
            # ETag ë¶ˆì¼ì¹˜ ë˜ëŠ” sha ë¯¸ê¸°ë¡ â†’ ì²˜ë¦¬ ëŒ€ìƒ
            to_process.append(fname)

        # 4) ì¶”ê°€/ì—…ë°ì´íŠ¸ ì²˜ë¦¬
        if to_process:
            print(f"\nâ• {len(to_process)}ê°œ íŒŒì¼ ì¸ë±ì‹±(ì¶”ê°€/ì—…ë°ì´íŠ¸) ì²˜ë¦¬.")
            blob_clients: List[BlobClient] = [self.container_client.get_blob_client(n) for n in to_process]
            new_docs = self.load_documents_from_blob(blob_clients)

            if new_docs:
                print("âš¡ ë¬¸ì„œ ë²¡í„° ì¸ë±ìŠ¤ì— upsert ì¤‘...")
                for doc in tqdm(new_docs, desc="ì¸ë±ì‹±"):
                    # LlamaIndexëŠ” ë™ì¼ ref_doc_id(doc.id_)ë¡œ insert ì‹œ ì—…ë°ì´íŠ¸ ì²˜ë¦¬
                    self.index.insert(doc)

                # ë©”íƒ€ ì €ì¥ì†Œ ì—…ë°ì´íŠ¸ (Blob props ê¸°ë°˜ + ë¡œë”© ë©”íƒ€ ê¸°ë°˜)
                for doc in new_docs:
                    fname = doc.metadata.get("file_name") or doc.id_
                    self.meta_store.set(fname, {
                        "etag": doc.metadata.get("etag", ""),
                        "sha256": doc.metadata.get("sha256", ""),
                        "last_modified": doc.metadata.get("last_modified", ""),
                    })
                self.meta_store.save()
                print("  âœ… ì¸ë±ì‹± ë° ë©”íƒ€ ì €ì¥ì†Œ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
        else:
            print("\nâœ… ë³€ê²½ ì‚¬í•­ ì—†ìŒ. ì¸ë±ìŠ¤ ìµœì‹  ìƒíƒœ.")

        print("=" * 64 + "\n")

    # -------------------------- ì§ˆì˜ --------------------------
    def query(self, question: str) -> str:
        """Azure AI Search ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•´ ì§ˆì˜"""
        print(f"ğŸ” '{self.index_name}' ì¸ë±ìŠ¤ ì§ˆì˜: {question[:64]}...")
        try:
            response = self.query_engine.query(question)
            return str(response)
        except Exception as e:
            return f"âŒ ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}"


# ============================== í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì„ íƒ) ==============================
def test_azure_rag_system():
    print("=" * 60)
    print("ğŸš€ Azure í†µí•© RAG ì‹œìŠ¤í…œ(Blob Storage + AI Search) ì¢…í•© í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    try:
        rag_system = AzureBlobRAGSystem(
            container_name="interview-data",
            index_name="sk-hynix-report-index",
        )

        # ì¸ë±ìŠ¤ ë™ê¸°í™” ì‹¤í–‰
        rag_system.sync_index(company_name_filter=None)

        # ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
        test_questions = [
            "ì‚¼ì„±ì „ìì˜ ì£¼ìš” ì‚¬ì—… ë¶€ë¬¸ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "íšŒì‚¬ì˜ í–¥í›„ ì „ë§ì€ ì–´ë–»ìŠµë‹ˆê¹Œ?",
        ]

        print("\nğŸ¯ ì‚¬ì—… í˜„í™© ì§ˆë¬¸ í…ŒìŠ¤íŠ¸:")
        for i, q in enumerate(test_questions, 1):
            print(f"\n[Q{i}] {q}")
            a = rag_system.query(q)
            print(f"[A{i}] {a}")

        print("\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_azure_rag_system()
