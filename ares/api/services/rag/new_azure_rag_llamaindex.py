# ares/api/services/rag/new_azure_rag_llamaindex.py
import os
import io
import json
import hashlib
import tempfile
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from datetime import datetime, timezone

from tqdm import tqdm
from bs4 import BeautifulSoup
from azure.core.credentials import AzureKeyCredential
from azure.storage.blob import BlobServiceClient, BlobClient, ContentSettings
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from xhtml2pdf import pisa

from llama_index.core import VectorStoreIndex, Settings, StorageContext, Document
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.azure_openai import AzureOpenAI
from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding
from llama_index.readers.file import PyMuPDFReader
from llama_index.vector_stores.azureaisearch import AzureAISearchVectorStore, IndexManagement

from ares.api.services.dart_service import DartService
from ares.api.services.blob_storage import BlobStorage
from ares.api.services.company_data import get_company_dart_name_map



# ============================== ë©”íƒ€ ì €ì¥ì†Œ ==============================
class _MetaStore:
    """
    ê°„ë‹¨í•œ íŒŒì¼ ê¸°ë°˜ ë©”íƒ€ ì €ì¥ì†Œ.
    êµ¬ì¡°:
    {
      "<blob_name>": {
        "etag": "...",
        "sha256": "...",
        "last_modified": "2024-09-01T09:00:00+00:00"
      },
      ...
    }
    """
    def __init__(self, path: Optional[str] = None):
        default_path = ".rag_meta.json"
        self.path = path or os.getenv("AZURE_RAG_META_PATH", default_path)
        self._data: Dict[str, Dict[str, str]] = {}
        self._load()

    def _load(self):
        try:
            if os.path.exists(self.path):
                with open(self.path, "r", encoding="utf-8") as f:
                    self._data = json.load(f)
            else:
                self._data = {}
        except Exception as e:
            print(f"âš ï¸ ë©”íƒ€ ì €ì¥ì†Œ ë¡œë“œ ì‹¤íŒ¨({self.path}): {e} â†’ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            self._data = {}

    def save(self):
        try:
            with open(self.path, "w", encoding="utf-8") as f:
                json.dump(self._data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ë©”íƒ€ ì €ì¥ì†Œ ì €ì¥ ì‹¤íŒ¨({self.path}): {e}")

    def get(self, key: str) -> Optional[Dict[str, str]]:
        return self._data.get(key)

    def set(self, key: str, value: Dict[str, str]):
        self._data[key] = value

    def delete(self, key: str):
        if key in self._data:
            del self._data[key]

    def keys(self):
        return list(self._data.keys())


# ============================== ìœ í‹¸ ==============================
def _sanitize_id(name: str) -> str:
    """íŒŒì¼ëª…ì„ ì¸ë±ìŠ¤ doc_idë¡œ ì‚¬ìš©í•  ë•Œ ì•ˆì „í•˜ê²Œ ë³€í™˜"""
    return name.replace("[", "_").replace("]", "_")


def _iso(dt: datetime) -> str:
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    return dt.astimezone(timezone.utc).isoformat()


def _calc_sha256_streaming(stream: io.BufferedReader, chunk_size: int = 1024 * 1024) -> str:
    """ëŒ€ìš©ëŸ‰ íŒŒì¼ì— ëŒ€í•´ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ sha256 ê³„ì‚°"""
    h = hashlib.sha256()
    while True:
        chunk = stream.read(chunk_size)
        if not chunk:
            break
        h.update(chunk)
    return h.hexdigest()


# ============================== ë³¸ì²´ ==============================
class AzureBlobRAGSystem:
    """Azure Blob + Azure AI Search ê¸°ë°˜ RAG ì‹œìŠ¤í…œ (ì¦ë¶„ ì¸ë±ì‹±: ETag/sha256/ë©”íƒ€ ì €ì¥)"""

    def __init__(self, container_name: str, index_name: str):
        print(f"ğŸš€ Azure í†µí•© RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì»¨í…Œì´ë„ˆ: {container_name}, ì¸ë±ìŠ¤: {index_name})...")
        self.container_name = container_name
        self.index_name = index_name
        self.query_engine = None

        self._require_env([
            "AZURE_OPENAI_ENDPOINT",
            "AZURE_OPENAI_KEY",
            "AZURE_STORAGE_CONNECTION_STRING",
            "AZURE_SEARCH_ENDPOINT",
            "AZURE_SEARCH_KEY",
        ])

        # Azure í´ë¼ì´ì–¸íŠ¸
        connect_str = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
        search_endpoint = os.getenv("AZURE_SEARCH_ENDPOINT")
        search_key = os.getenv("AZURE_SEARCH_KEY")
        credential = AzureKeyCredential(search_key)

        self.blob_service_client = BlobServiceClient.from_connection_string(connect_str)
        self.container_client = self.blob_service_client.get_container_client(self.container_name)

        self.search_client = SearchClient(endpoint=search_endpoint, index_name=self.index_name, credential=credential)
        self.search_index_client = SearchIndexClient(endpoint=search_endpoint, credential=credential)

        # DART ì—°ë™ ë° ê¸°ì—… ë°ì´í„° ì„œë¹„ìŠ¤
        try:
            self.dart_service = DartService()
            self.blob_storage = BlobStorage()
            self.company_data = get_company_dart_name_map()
            print("âœ… DART ì„œë¹„ìŠ¤ ë° ê¸°ì—… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ DART ì„œë¹„ìŠ¤ ë˜ëŠ” ê¸°ì—… ë°ì´í„° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.dart_service = None
            self.company_data = {}


        # LlamaIndex ì„¤ì •
        self._setup_llamaindex()

        # ë²¡í„° ìŠ¤í† ì–´/ì¸ë±ìŠ¤/ì¿¼ë¦¬ì—”ì§„
        try:
            self.vector_store = AzureAISearchVectorStore(
                search_or_index_client=self.search_index_client,
                index_name=self.index_name,
                id_field_key="id",
                chunk_field_key="chunk",
                embedding_field_key="embedding",
                metadata_string_field_key="metadata",
                doc_id_field_key="doc_id",
                index_management=IndexManagement.CREATE_IF_NOT_EXISTS,
            )
            self.storage_context = StorageContext.from_defaults(vector_store=self.vector_store)
            self.index = VectorStoreIndex.from_vector_store(self.vector_store)
            self.query_engine = self.index.as_query_engine(similarity_top_k=5)
            print("âœ… Azure AI Search VectorStore ë° ì¿¼ë¦¬ ì—”ì§„ ì„¤ì • ì™„ë£Œ")
        except Exception as e:
            raise ConnectionError(f"Azure AI Search VectorStore ì„¤ì • ì‹¤íŒ¨: {e}")

        # ë©”íƒ€ ì €ì¥ì†Œ
        self.meta_store = _MetaStore()

    def is_ready(self) -> bool:
        """RAG ì‹œìŠ¤í…œì´ ì¿¼ë¦¬ë¥¼ ìˆ˜í–‰í•  ì¤€ë¹„ê°€ ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        return self.query_engine is not None

    # -------------------------- ë‚´ë¶€ ì„¤ì • --------------------------
    def _require_env(self, keys: List[str]):
        missing = [k for k in keys if not os.getenv(k)]
        if missing:
            raise ValueError(f"í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ ëˆ„ë½: {', '.join(missing)}")

    def _setup_llamaindex(self):
        print("ğŸ”§ LlamaIndex êµ¬ì„± ìš”ì†Œ ì„¤ì • ì¤‘...")
        Settings.llm = AzureOpenAI(
            engine=os.getenv("AZURE_OPENAI_MODEL"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            api_key=os.getenv("AZURE_OPENAI_KEY"),
            api_version=os.getenv("API_VERSION", "2024-02-15-preview"),
        )
        
        # --- [DEBUG] ì„ë² ë”© ì„¤ì • ê°’ ì¶œë ¥ ---
        embedding_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
        embedding_key = os.getenv("AZURE_OPENAI_KEY")
        embedding_deployment = os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT", "text-embedding-3-small")
        print("\n--- [DEBUG] Azure OpenAI Embedding Settings ---")
        print(f"  - ENDPOINT: {embedding_endpoint}")
        print(f"  - API KEY: {'*' * (len(embedding_key) - 4) + embedding_key[-4:] if embedding_key else 'Not Set'}")
        print(f"  - DEPLOYMENT: {embedding_deployment}")
        print("---------------------------------------------\n")
        
        Settings.embed_model = AzureOpenAIEmbedding(
            model=embedding_deployment,
            deployment_name=embedding_deployment,
            azure_endpoint=embedding_endpoint,
            api_key=embedding_key,
            api_version=os.getenv("API_VERSION", "2024-02-15-preview"),
        )
        Settings.node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=200)
        print("  âœ… LlamaIndex ì„¤ì • ì™„ë£Œ")

    # -------------------------- ì¸ë±ìŠ¤ ë©”íƒ€ ì¡°íšŒ --------------------------
    def _get_indexed_doc_metadata(self) -> Dict[str, Dict[str, str]]:
        """
        AI Search ì¸ë±ìŠ¤ì— ì´ë¯¸ ë“¤ì–´ìˆëŠ” ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¨ë‹¤.
        ë°˜í™˜: { doc_id: {"last_modified": "...", "etag": "...", "sha256": "..."} }
        """
        print("ğŸ“Š AI Search ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì¤‘...")
        indexed: Dict[str, Dict[str, str]] = {}
        try:
            # metadataëŠ” stringified JSONìœ¼ë¡œ ì €ì¥ë˜ì–´ ìˆìŒ
            results = self.search_client.search(search_text="*", select=["doc_id", "metadata"])
            for doc in results:
                try:
                    meta = json.loads(doc.get("metadata", "{}"))
                    indexed[doc["doc_id"]] = {
                        "last_modified": meta.get("last_modified", ""),
                        "etag": meta.get("etag", ""),
                        "sha256": meta.get("sha256", ""),
                    }
                except Exception:
                    continue
            print(f"  âœ… ì¸ë±ìŠ¤ì— {len(indexed)}ê°œ ë¬¸ì„œ ë©”íƒ€ ìˆ˜ì§‘.")
        except Exception as e:
            print(f"  âš ï¸ ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨(ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŒ): {e}")
        return indexed

    # -------------------------- Blob â†’ Document ë¡œë”© --------------------------
    def _download_to_temp_and_hash(self, blob_client: BlobClient) -> Tuple[str, str]:
        """
        Blobì„ ì„ì‹œíŒŒì¼ë¡œ ì €ì¥í•˜ê³  sha256ì„ ê³„ì‚°í•œë‹¤.
        ë°˜í™˜: (temp_filepath, sha256_hex)
        """
        props = blob_client.get_blob_properties()
        total = props.size or None

        # ì„ì‹œíŒŒì¼ ìƒì„±
        suffix = Path(blob_client.blob_name).suffix
        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
        tmp_path = tmp.name

        sha256 = hashlib.sha256()
        try:
            stream = blob_client.download_blob()
            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì½ì–´ ì„ì‹œíŒŒì¼ì— ì“°ê³  ë™ì‹œì— í•´ì‹œ ê³„ì‚°
            for chunk in stream.chunks():
                if not isinstance(chunk, (bytes, bytearray)):
                    chunk = chunk.readall()
                tmp.write(chunk)
                sha256.update(chunk)
            tmp.close()
            return tmp_path, sha256.hexdigest()
        except Exception as e:
            try:
                tmp.close()
            except Exception:
                pass
            try:
                if os.path.exists(tmp_path):
                    os.remove(tmp_path)
            except Exception:
                pass
            raise RuntimeError(f"Blob ë‹¤ìš´ë¡œë“œ/í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {blob_client.blob_name}, {e}")

    def load_documents_from_blob(self, blobs_to_process: List[BlobClient]) -> List[Document]:
        """
        ì§€ì •ëœ Blob ëª©ë¡ì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ì—¬ LlamaIndex Document ê°ì²´ë¡œ ë³€í™˜.
        ê° Document.metadataì— file_name/container/source/last_modified/etag/sha256 í¬í•¨.
        """
        if not blobs_to_process:
            return []

        documents: List[Document] = []
        pdf_reader = PyMuPDFReader()
        print("ğŸ“– ì§€ì •ëœ Blobì—ì„œ ë¬¸ì„œ ë¡œë”© ì‹œì‘...")

        for blob_client in blobs_to_process:
            blob_name = blob_client.blob_name
            print(f"  ğŸ” ì²˜ë¦¬ ì¤‘: {blob_name}")

            try:
                props = blob_client.get_blob_properties()
                etag = getattr(props, "etag", "") or ""
                last_modified = props.last_modified.replace(tzinfo=timezone.utc)

                # ë‹¤ìš´ë¡œë“œ + í•´ì‹œ
                temp_path, sha256_hex = self._download_to_temp_and_hash(blob_client)

                try:
                    # íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ë¡œë”©
                    if blob_name.lower().endswith(".pdf"):
                        loaded_docs = pdf_reader.load_data(file_path=temp_path)
                    elif blob_name.lower().endswith(".txt"):
                        text = Path(temp_path).read_text(encoding="utf-8", errors="ignore")
                        loaded_docs = [Document(text=text)]
                    elif blob_name.lower().endswith(".xml"):
                        # XML íŒŒì¼ ì²˜ë¦¬: BeautifulSoupìœ¼ë¡œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                        raw_content = Path(temp_path).read_bytes()
                        soup = BeautifulSoup(raw_content, 'lxml')
                        text = soup.get_text(separator='\n', strip=True)
                        loaded_docs = [Document(text=text)]
                    else:
                        print("    â„¹ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” í™•ì¥ì. ìŠ¤í‚µ:", blob_name)
                        continue

                    # ë©”íƒ€ë°ì´í„° ë¶€ì—¬
                    for doc in loaded_docs:
                        doc.id_ = _sanitize_id(blob_name)
                        doc.metadata.update({
                            "file_name": _sanitize_id(blob_name),
                            "container": self.container_name,
                            "source": "azure_blob",
                            "last_modified": _iso(last_modified),
                            "etag": etag,
                            "sha256": sha256_hex,
                        })

                    documents.extend(loaded_docs)
                    print(f"    âœ… '{blob_name}' ë¡œë“œ ì™„ë£Œ ({len(loaded_docs)}ê°œ ë¬¸ì„œ)")

                finally:
                    # ì„ì‹œíŒŒì¼ ì œê±°
                    try:
                        os.remove(temp_path)
                    except Exception:
                        pass

            except Exception as e:
                print(f"    âŒ '{blob_name}' ë¡œë“œ ì‹¤íŒ¨: {e}")

        print(f"âœ… ì´ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ.")
        return documents

    # -------------------------- ì‚­ì œ --------------------------
    def delete_doc(self, doc_id: str):
        """AI Searchì—ì„œ íŠ¹ì • ë¬¸ì„œ IDì™€ ê´€ë ¨ëœ ëª¨ë“  ì²­í¬ ì‚­ì œ"""
        print(f"ğŸ—‘ï¸ ì¸ë±ìŠ¤ì—ì„œ '{doc_id}' ë¬¸ì„œ ì‚­ì œ ì¤‘...")
        try:
            self.index.delete_ref_doc(ref_doc_id=doc_id, delete_from_docstore=True)
            print(f"  âœ… '{doc_id}' ì‚­ì œ ì™„ë£Œ.")
        except Exception as e:
            print(f"  âŒ '{doc_id}' ì‚­ì œ ì‹¤íŒ¨: {e}")

    # -------------------------- ë™ê¸°í™” --------------------------
    def _ensure_latest_report_from_dart(self, company_name: str):
        """DART APIë¥¼ í†µí•´ ìµœì‹  ì‚¬ì—…ë³´ê³ ì„œë¥¼ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œí•˜ì—¬ Blob Storageì— ì—…ë¡œë“œ"""
        if not self.dart_service or not self.company_data:
            print("  âš ï¸ DART ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ìµœì‹  ë³´ê³ ì„œ í™•ì¸ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        print(f"ğŸ¯ DARTì—ì„œ '{company_name}'ì˜ ìµœì‹  ì‚¬ì—…ë³´ê³ ì„œ í™•ì¸ ì¤‘...")
        
        exact_company_name = self.company_data.get(company_name)
        if not exact_company_name:
            print(f"  âš ï¸ ê¸°ì—… ë°ì´í„°ì—ì„œ '{company_name}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        corp_code = self.dart_service.get_corp_code(exact_company_name)
        if not corp_code:
            print(f"  âš ï¸ DARTì—ì„œ '{exact_company_name}'ì˜ ê¸°ì—… ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        report_info = self.dart_service.get_latest_business_report_info(corp_code)
        if not report_info:
            print(f"  â„¹ï¸ '{exact_company_name}'ì˜ ìµœì‹  ì‚¬ì—…ë³´ê³ ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        rcept_no = report_info.get("rcept_no")
        if not rcept_no:
            print(f"  âš ï¸ ë³´ê³ ì„œ ì •ë³´ì— ì ‘ìˆ˜ë²ˆí˜¸ê°€ ì—†ìŠµë‹ˆë‹¤: {report_info}")
            return

        blob_name_xml = f"[{company_name}]ì‚¬ì—…ë³´ê³ ì„œ_{rcept_no}.xml"
        if self.blob_storage.blob_exists(blob_name_xml):
            print(f"  âœ… ìµœì‹  ì‚¬ì—…ë³´ê³ ì„œ '{blob_name_xml}'ì´(ê°€) ì´ë¯¸ Blob Storageì— ì¡´ì¬í•©ë‹ˆë‹¤.")
            return

        print(f"  ğŸ“¥ '{blob_name_xml}' ë‹¤ìš´ë¡œë“œ ë° ì—…ë¡œë“œ ì‹œì‘...")
        xml_content_bytes = self.dart_service.download_document(rcept_no)
        if xml_content_bytes:
            try:
                self.blob_storage.upload_blob(blob_name_xml, xml_content_bytes, "application/xml")
                print(f"  âœ… '{blob_name_xml}'ì„(ë¥¼) Blob Storageì— ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"  âŒ Blob Storage ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            print(f"  âŒ DARTì—ì„œ ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ì ‘ìˆ˜ë²ˆí˜¸: {rcept_no})")


    def sync_index(self, company_name_filter: Optional[str] = None):
        """
        Blob â†” ì¸ë±ìŠ¤ ì¦ë¶„ ë™ê¸°í™”.
        - DART APIë¥¼ í†µí•´ ìµœì‹  ë³´ê³ ì„œê°€ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ (í•„í„°ë§ ì‹œ)
        - ì¶”ê°€/ë³€ê²½ íŒë‹¨ ê¸°ì¤€: ETag + sha256 (ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ë³€ê²½ ì‹œ ì—…ë°ì´íŠ¸)
        - ë©”íƒ€ ì €ì¥ì†Œì™€ë„ ë™ê¸°í™” (ë¡œì»¬ JSON)
        - ì‚­ì œ: Blob/ë©”íƒ€/ì¸ë±ìŠ¤ ê°„ ë¶ˆì¼ì¹˜ ì •ë¦¬
        """
        print("\n" + "=" * 64)
        print("ğŸ”„ Azure AI Search ì¸ë±ìŠ¤ ì¦ë¶„ ë™ê¸°í™” ì‹œì‘...")

        # DART APIë¥¼ í†µí•´ ìµœì‹  ì‚¬ì—…ë³´ê³ ì„œ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ (í•„í„°ë§ ì‹œ)
        if company_name_filter:
            self._ensure_latest_report_from_dart(company_name_filter)

        # 0) ì†ŒìŠ¤ ë‚˜ì—´
        source_blobs: Dict[str, Dict[str, str]] = {}
        for blob in self.container_client.list_blobs():
            name = blob.name
            # DARTì—ì„œ ë‹¤ìš´ë¡œë“œí•œ xml íŒŒì¼ë„ ì²˜ë¦¬ ëŒ€ìƒì— í¬í•¨
            if not name.lower().endswith((".pdf", ".txt", ".xml")):
                continue
            if company_name_filter:
                expected_prefix = f"[{company_name_filter}]"
                if not name.startswith(expected_prefix):
                    continue
            source_blobs[name] = {
                "last_modified": _iso(blob.last_modified.replace(tzinfo=timezone.utc)),
                "etag": getattr(blob, "etag", "") or "",
            }

        # 1) ì¸ë±ìŠ¤/ë©”íƒ€ ì €ì¥ì†Œ í˜„í™©
        indexed_docs = self._get_indexed_doc_metadata()            # {doc_id: {...}}
        meta_keys = set(self.meta_store.keys())                    # blob_name ì§‘í•©
        source_set = set(source_blobs.keys())
        indexed_set = set(indexed_docs.keys())

        # 2) ì‚­ì œ ëŒ€ìƒ
        to_delete_in_index = indexed_set - source_set              # ì¸ë±ìŠ¤ì—ëŠ” ìˆìœ¼ë‚˜ Blobì— ì—†ëŠ” ê²ƒ
        to_delete_in_meta = meta_keys - source_set                 # ë©”íƒ€ì—ëŠ” ìˆìœ¼ë‚˜ Blobì— ì—†ëŠ” ê²ƒ

        if to_delete_in_index:
            print(f"\nğŸ§¹ Blobì— ì—†ëŠ” {len(to_delete_in_index)}ê°œë¥¼ ì¸ë±ìŠ¤ì—ì„œ ì‚­ì œí•©ë‹ˆë‹¤.")
            for fname in to_delete_in_index:
                self.delete_doc(fname)

        if to_delete_in_meta:
            print(f"\nğŸ§¹ Blobì— ì—†ëŠ” {len(to_delete_in_meta)}ê°œë¥¼ ë©”íƒ€ ì €ì¥ì†Œì—ì„œ ì •ë¦¬í•©ë‹ˆë‹¤.")
            for fname in to_delete_in_meta:
                self.meta_store.delete(fname)

        # 3) ë³€ê²½/ì¶”ê°€ íŒì •
        to_process: List[str] = []
        for fname, src_meta in source_blobs.items():
            src_etag = src_meta.get("etag", "")
            meta_entry = self.meta_store.get(fname) or {}
            meta_etag = meta_entry.get("etag", "")
            meta_sha = meta_entry.get("sha256", "")

            # ì¸ë±ìŠ¤ ë©”íƒ€(ì°¸ê³ )
            idx_entry = indexed_docs.get(fname) or {}
            idx_etag = idx_entry.get("etag", "")
            idx_sha = idx_entry.get("sha256", "")

            # ìš°ì„ ìˆœìœ„: ETagê°€ ë‹¤ë¥´ë©´ ë‹¤ìš´ë¡œë“œ/í•´ì‹œ í›„ ë¹„êµ â†’ sha ë³€ê²½ ì—¬ë¶€ ìµœì¢…íŒì •
            if src_etag and src_etag == meta_etag and meta_sha and meta_sha == idx_sha:
                # ì†ŒìŠ¤ ETag = ë©”íƒ€ ETag = ì¸ë±ìŠ¤ sha ë™ì¼ â†’ ìŠ¤í‚µ
                continue
            # ETag ë¶ˆì¼ì¹˜ ë˜ëŠ” sha ë¯¸ê¸°ë¡ â†’ ì²˜ë¦¬ ëŒ€ìƒ
            to_process.append(fname)

        # 4) ì¶”ê°€/ì—…ë°ì´íŠ¸ ì²˜ë¦¬
        if to_process:
            print(f"\nâ• {len(to_process)}ê°œ íŒŒì¼ ì¸ë±ì‹±(ì¶”ê°€/ì—…ë°ì´íŠ¸) ì²˜ë¦¬.")
            blob_clients: List[BlobClient] = [self.container_client.get_blob_client(n) for n in to_process]
            new_docs = self.load_documents_from_blob(blob_clients)

            if new_docs:
                print("âš¡ ë¬¸ì„œ ë²¡í„° ì¸ë±ìŠ¤ì— upsert ì¤‘...")
                node_parser = Settings.node_parser
                for doc in new_docs:
                    print(f"  - ë¬¸ì„œ '{doc.id_}' ë…¸ë“œ ë¶„í•  ì¤‘...")
                    nodes = node_parser.get_nodes_from_documents([doc])
                    print(f"  - '{doc.id_}'ì—ì„œ {len(nodes)}ê°œì˜ ë…¸ë“œ ìƒì„±. 50ê°œì”© ë°°ì¹˜í•˜ì—¬ ì¸ë±ì‹±í•©ë‹ˆë‹¤.")
                    
                    # 50ê°œì”© ë°°ì¹˜ë¡œ ì¸ë±ì‹±
                    batch_size = 50
                    for i in tqdm(range(0, len(nodes), batch_size), desc=f"'{doc.id_}' ì¸ë±ì‹±"):
                        batch = nodes[i:i+batch_size]
                        self.index.insert_nodes(batch)
                    
                    print(f"  - ë¬¸ì„œ '{doc.id_}' ì¸ë±ì‹± ì™„ë£Œ.")

                # ë©”íƒ€ ì €ì¥ì†Œ ì—…ë°ì´íŠ¸ (Blob props ê¸°ë°˜ + ë¡œë”© ë©”íƒ€ ê¸°ë°˜)
                for doc in new_docs:
                    fname = doc.metadata.get("file_name") or doc.id_
                    self.meta_store.set(fname, {
                        "etag": doc.metadata.get("etag", ""),
                        "sha256": doc.metadata.get("sha256", ""),
                        "last_modified": doc.metadata.get("last_modified", ""),
                    })
                self.meta_store.save()
                print("  âœ… ì¸ë±ì‹± ë° ë©”íƒ€ ì €ì¥ì†Œ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
        else:
            print("\nâœ… ë³€ê²½ ì‚¬í•­ ì—†ìŒ. ì¸ë±ìŠ¤ ìµœì‹  ìƒíƒœ.")

        print("=" * 64 + "\n")

    # -------------------------- ì§ˆì˜ --------------------------
    def query(self, question: str) -> str:
        """Azure AI Search ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•´ ì§ˆì˜"""
        print(f"ğŸ” '{self.index_name}' ì¸ë±ìŠ¤ ì§ˆì˜: {question[:64]}...")
        try:
            response = self.query_engine.query(question)
            return str(response)
        except Exception as e:
            return f"âŒ ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}"


# ============================== í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì„ íƒ) ==============================
def test_azure_rag_system():
    print("=" * 60)
    print("ğŸš€ Azure í†µí•© RAG ì‹œìŠ¤í…œ(Blob Storage + AI Search) ì¢…í•© í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    try:
        rag_system = AzureBlobRAGSystem(
            container_name="interview-data",
            index_name="sk-hynix-report-index",
        )

        # ì¸ë±ìŠ¤ ë™ê¸°í™” ì‹¤í–‰
        rag_system.sync_index(company_name_filter=None)

        # ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
        test_questions = [
            "ì‚¼ì„±ì „ìì˜ ì£¼ìš” ì‚¬ì—… ë¶€ë¬¸ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "íšŒì‚¬ì˜ í–¥í›„ ì „ë§ì€ ì–´ë–»ìŠµë‹ˆê¹Œ?",
        ]

        print("\nğŸ¯ ì‚¬ì—… í˜„í™© ì§ˆë¬¸ í…ŒìŠ¤íŠ¸:")
        for i, q in enumerate(test_questions, 1):
            print(f"\n[Q{i}] {q}")
            a = rag_system.query(q)
            print(f"[A{i}] {a}")

        print("\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_azure_rag_system()

