import os
import tempfile
import json
from pathlib import Path
from typing import List, Dict
from datetime import datetime, timezone
from dotenv import load_dotenv
from tqdm import tqdm

# --- í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸  --- 
from azure.storage.blob import BlobServiceClient, BlobClient
from llama_index.core import VectorStoreIndex, Settings, StorageContext, Document
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.azure_openai import AzureOpenAI
from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding
from llama_index.readers.file import PyMuPDFReader
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from llama_index.vector_stores.azureaisearch import AzureAISearchVectorStore, IndexManagement

class AzureBlobRAGSystem:
    """Azure Blob Storage ë¬¸ì„œì™€ Azure AI Search ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” RAG ì‹œìŠ¤í…œ (ì¦ë¶„ ì¸ë±ì‹± ì§€ì›)"""

    def __init__(self, container_name: str, index_name: str):
        print(f"ğŸš€ Azure í†µí•© RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì»¨í…Œì´ë„ˆ: {container_name}, AI Search ì¸ë±ìŠ¤: {index_name})...")
        
        self.container_name = container_name
        self.index_name = index_name
        
        self._load_env()
        
        # Azure ì„œë¹„ìŠ¤ í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        connect_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')
        search_endpoint = os.getenv("AZURE_SEARCH_ENDPOINT")
        search_key = os.getenv("AZURE_SEARCH_KEY")
        credential = AzureKeyCredential(search_key)
        
        self.blob_service_client = BlobServiceClient.from_connection_string(connect_str)
        self.container_client = self.blob_service_client.get_container_client(self.container_name)
        
        # ë¬¸ì„œ ì¡°íšŒë¥¼ ìœ„í•œ SearchClient
        self.search_client = SearchClient(endpoint=search_endpoint, index_name=self.index_name, credential=credential)
        
        # ì¸ë±ìŠ¤ ê´€ë¦¬ë¥¼ ìœ„í•œ SearchIndexClient
        search_index_client = SearchIndexClient(endpoint=search_endpoint, credential=credential)
        
        self._setup_llamaindex()
        
        try:
            self.vector_store = AzureAISearchVectorStore(
                search_or_index_client=search_index_client,
                index_name=self.index_name,
                id_field_key="id",
                chunk_field_key="chunk",
                embedding_field_key="embedding",
                metadata_string_field_key="metadata",
                doc_id_field_key="doc_id", # ë¬¸ì„œ ì‹ë³„ì„ ìœ„í•´ í•„ìˆ˜
                index_management=IndexManagement.CREATE_IF_NOT_EXISTS,
            )
            self.storage_context = StorageContext.from_defaults(vector_store=self.vector_store)
            
            # ì¿¼ë¦¬ë¥¼ ìœ„í•œ ì¸ë±ìŠ¤ ë° ì¿¼ë¦¬ ì—”ì§„ ì´ˆê¸°í™”
            self.index = VectorStoreIndex.from_vector_store(self.vector_store)
            self.query_engine = self.index.as_query_engine(similarity_top_k=5)
            print("âœ… Azure AI Search VectorStore ë° ì¿¼ë¦¬ ì—”ì§„ ì„¤ì • ì™„ë£Œ")

        except Exception as e:
            raise ConnectionError(f"Azure AI Search VectorStore ì„¤ì • ì‹¤íŒ¨: {e}")

    def _sanitize_id(self, name: str) -> str:
        """íŒŒì¼ ì´ë¦„ì˜ íŠ¹ìˆ˜ ë¬¸ìë¥¼ IDë¡œ ì‚¬ìš©í•˜ê¸° ì•ˆì „í•˜ê²Œ ë³€ê²½"""
        return name.replace('[', '_').replace(']', '_')

    def _load_env(self):
        """í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ"""
        
        required_vars = ['AZURE_OPENAI_ENDPOINT', 'AZURE_OPENAI_KEY', 'AZURE_STORAGE_CONNECTION_STRING', 'AZURE_SEARCH_ENDPOINT', 'AZURE_SEARCH_KEY']
        if not all(os.getenv(var) for var in required_vars):
            raise ValueError("í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ê°€ .env.keys íŒŒì¼ì— ëª¨ë‘ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    def _setup_llamaindex(self):
        """LlamaIndexì˜ LLM, ì„ë² ë”© ëª¨ë¸ ë“± ì„¤ì •"""
        print("ğŸ”§ LlamaIndex êµ¬ì„± ìš”ì†Œ ì„¤ì • ì¤‘...")
        Settings.llm = AzureOpenAI(
            engine=os.getenv('AZURE_OPENAI_MODEL'),
            azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),
            api_key=os.getenv('AZURE_OPENAI_KEY'),
            api_version=os.getenv('API_VERSION', '2024-02-15-preview'),
        )
        Settings.embed_model = AzureOpenAIEmbedding(
            model="text-embedding-3-small",
            deployment_name=os.getenv('AZURE_EMBEDDING_MODEL', 'text-embedding-3-small'),
            azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),
            api_key=os.getenv('AZURE_OPENAI_KEY'),
            api_version='2023-05-15'
        )
        Settings.node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=200)
        print("  âœ… LlamaIndex ì„¤ì • ì™„ë£Œ")

    def _get_indexed_doc_metadata(self) -> Dict[str, datetime]:
        """AI Searchì— ì €ì¥ëœ ë¬¸ì„œë“¤ì˜ íŒŒì¼ëª…ê³¼ ë§ˆì§€ë§‰ ìˆ˜ì • ì‹œê°„ì„ ê°€ì ¸ì˜¨ë‹¤."""
        print("ğŸ“Š AI Searchì—ì„œ ê¸°ì¡´ ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì¤‘...")
        indexed_docs = {}
        try:
            # 'doc_id'ì™€ 'metadata' í•„ë“œë§Œ ì„ íƒí•˜ì—¬ ëª¨ë“  ë¬¸ì„œ ê²€ìƒ‰
            results = self.search_client.search(search_text="*", select=["doc_id", "metadata"])
            for doc in results:
                metadata = json.loads(doc.get("metadata", "{}"))
                last_modified_str = metadata.get("last_modified")
                if last_modified_str:
                    # ISO í˜•ì‹ì˜ ë¬¸ìì—´ì„ timezone-aware datetime ê°ì²´ë¡œ ë³€í™˜
                    indexed_docs[doc["doc_id"]] = datetime.fromisoformat(last_modified_str)
            print(f"  âœ… ì´ {len(indexed_docs)}ê°œì˜ ì¸ë±ì‹±ëœ ë¬¸ì„œ ì •ë³´ í™•ì¸.")
        except Exception as e:
            print(f"  âš ï¸ ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨ (ì¸ë±ìŠ¤ê°€ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŒ): {e}")
        return indexed_docs

    def load_documents_from_blob(self, blobs_to_process: List[BlobClient]) -> List[Document]:
        """ì§€ì •ëœ Azure Blob ëª©ë¡ì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ì—¬ LlamaIndex Document ê°ì²´ë¡œ ë³€í™˜"""
        if not blobs_to_process:
            return []

        documents = []
        pdf_reader = PyMuPDFReader()
        print("ğŸ“– ì§€ì •ëœ Blobì—ì„œ ë¬¸ì„œ ë¡œë”© ì‹œì‘...")

        for blob_client in blobs_to_process:
            blob_name = blob_client.blob_name
            print(f"  ğŸ” ì²˜ë¦¬ ì¤‘: {blob_name}")
            
            # 1. with êµ¬ë¬¸ ë°–ì—ì„œ delete=Falseë¡œ ì„ì‹œ íŒŒì¼ ìƒì„±
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=Path(blob_name).suffix)
            try:
                downloader = blob_client.download_blob()
                temp_file.write(downloader.readall())
                
                # 2. â˜…â˜…â˜…ê°€ì¥ ì¤‘ìš”â˜…â˜…â˜… íŒŒì¼ì„ ëª…ì‹œì ìœ¼ë¡œ ë‹«ì•„ ì ê¸ˆì„ í•´ì œí•©ë‹ˆë‹¤.
                temp_file.close()

                # 3. ì´ì œ ì ê¸ˆì´ í’€ë¦° íŒŒì¼ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
                if blob_name.lower().endswith('.pdf'):
                    loaded_docs = pdf_reader.load_data(file_path=temp_file.name)
                elif blob_name.lower().endswith('.txt'):
                    text_content = Path(temp_file.name).read_text(encoding='utf-8')
                    loaded_docs = [Document(text=text_content)]
                else:
                    continue
                
                last_modified_utc = blob_client.get_blob_properties().last_modified.replace(tzinfo=timezone.utc)

                for doc in loaded_docs:
                    doc.id_ = self._sanitize_id(blob_name) 
                    doc.metadata.update({
                        'file_name': self._sanitize_id(blob_name),
                        'container': self.container_name,
                        'source': 'azure_blob',
                        'last_modified': last_modified_utc.isoformat()
                    })
                documents.extend(loaded_docs)
                print(f"    âœ… '{blob_name}' ë¡œë“œ ì™„ë£Œ ({len(loaded_docs)}ê°œ ë¬¸ì„œ)")

            except Exception as e:
                print(f"    âŒ '{blob_name}' ë¡œë“œ ì‹¤íŒ¨: {e}")
            finally:
                # 4. ì‘ì—…ì´ ì„±ê³µí•˜ë“  ì‹¤íŒ¨í•˜ë“ , ì„ì‹œ íŒŒì¼ì„ ë°˜ë“œì‹œ ì‚­ì œí•©ë‹ˆë‹¤.
                os.remove(temp_file.name)
        
        print(f"âœ… ì´ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ.")
        return documents


    def delete_doc(self, doc_id: str):
        """AI Searchì—ì„œ íŠ¹ì • ë¬¸ì„œ IDì™€ ê´€ë ¨ëœ ëª¨ë“  ì²­í¬ë¥¼ ì‚­ì œ"""
        print(f"ğŸ—‘ï¸ ì¸ë±ìŠ¤ì—ì„œ '{doc_id}' ë¬¸ì„œ ì‚­ì œ ì¤‘...")
        try:
            # ref_doc_idë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ë…¸ë“œ(ì²­í¬) ì‚­ì œ
            self.index.delete_ref_doc(ref_doc_id=doc_id, delete_from_docstore=True)
            print(f"  âœ… '{doc_id}' ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ.")
        except Exception as e:
            print(f"  âŒ '{doc_id}' ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {e}")

    def sync_index(self, company_name_filter: str = None):
        """Blob Storageì™€ AI Search ì¸ë±ìŠ¤ë¥¼ ë™ê¸°í™” (ì¶”ê°€/ì—…ë°ì´íŠ¸/ì‚­ì œ)"""
        print("\n" + "="*60)
        print("ğŸ”„ Azure AI Search ì¸ë±ìŠ¤ ë™ê¸°í™” ì‹œì‘...")
        
        # 1. ì†ŒìŠ¤(Blob Storage)ì˜ í˜„ì¬ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
        source_blobs = {}
        for blob in self.container_client.list_blobs():
            if blob.name.endswith(('.pdf', '.txt')):
                if company_name_filter:
                    # Construct the expected prefix for the company
                    expected_prefix = f"[{company_name_filter}]"
                    if not blob.name.startswith(expected_prefix):
                        continue # Skip blobs that don't match the company filter
                source_blobs[blob.name] = blob.last_modified.replace(tzinfo=timezone.utc)
        
        # 2. ëŒ€ìƒ(AI Search)ì˜ í˜„ì¬ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
        indexed_docs = self._get_indexed_doc_metadata()
        
        # 3. ë³€ê²½ ì‚¬í•­ ê³„ì‚°
        source_files = set(source_blobs.keys())
        indexed_files = set(indexed_docs.keys())
        
        files_to_add = source_files - indexed_files
        files_to_delete = indexed_files - source_files
        files_to_check = indexed_files.intersection(source_files)
        
        files_to_update = {
            fname for fname in files_to_check 
            if source_blobs[fname] > indexed_docs[fname]
        }
        
        # 4. ë³€ê²½ ì‚¬í•­ ì ìš©
        # (4-1) ì‚­ì œ
        if files_to_delete:
            print(f"\n {len(files_to_delete)}ê°œì˜ ì‚­ì œí•  íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
            for fname in files_to_delete:
                self.delete_doc(fname)
        
        # (4-2) ì—…ë°ì´íŠ¸ (ì‚­ì œ í›„ ì¶”ê°€)
        if files_to_update:
            print(f"\nğŸ”„ {len(files_to_update)}ê°œì˜ ì—…ë°ì´íŠ¸í•  íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
            for fname in files_to_update:
                self.delete_doc(fname) # ê¸°ì¡´ ë²„ì „ ì‚­ì œ

        # (4-3) ì¶”ê°€ (ìƒˆ íŒŒì¼ + ì—…ë°ì´íŠ¸ëœ íŒŒì¼)
        files_to_process = files_to_add.union(files_to_update)
        if files_to_process:
            print(f"\nâ• {len(files_to_process)}ê°œì˜ ì¶”ê°€/ì—…ë°ì´íŠ¸í•  íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            blob_clients_to_process = [
                self.container_client.get_blob_client(fname) for fname in files_to_process
            ]
            
            new_documents = self.load_documents_from_blob(blob_clients_to_process)
            
            if new_documents:
                print("âš¡ ìƒˆë¡œìš´ ë¬¸ì„œë¥¼ ë²¡í„° ì¸ë±ìŠ¤ì— ì¶”ê°€í•˜ëŠ” ì¤‘...")
                # âœ… ìˆ˜ì •ëœ ë¶€ë¶„: for ë°˜ë³µë¬¸ìœ¼ë¡œ ë¬¸ì„œë¥¼ í•˜ë‚˜ì”© insert í•©ë‹ˆë‹¤.
                for doc in tqdm(new_documents, desc="ìƒˆ ë¬¸ì„œ ì¸ë±ì‹±"):
                    self.index.insert(doc)
                print("  âœ… ìƒˆë¡œìš´ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ.")
        
        if not any([files_to_add, files_to_delete, files_to_update]):
            print("\nâœ… ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤. ë³€ê²½ ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        print("="*60 + "\n")

    def query(self, question: str) -> str:
        """Azure AI Search ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€"""
        print(f"ğŸ” '{self.index_name}' ì¸ë±ìŠ¤ì—ì„œ ì§ˆë¬¸ ì²˜ë¦¬: {question[:50]}...")
        try:
            response = self.query_engine.query(question)
            return str(response)
        except Exception as e:
            return f"âŒ ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}"

# --- ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì‹¤í–‰ ì½”ë“œ ---
def test_azure_rag_system():
    print("=" * 60)
    print("ğŸš€ Azure í†µí•© RAG ì‹œìŠ¤í…œ(Blob Storage + AI Search) ì¢…í•© í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    try:
        rag_system = AzureBlobRAGSystem(
            container_name='interview-data', 
            index_name='sk-hynix-report-index'
        )

        # ì¸ë±ìŠ¤ ë™ê¸°í™” ì‹¤í–‰
        rag_system.sync_index()

        # ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
        test_questions = [
            "ì‚¼ì„±ì „ìì˜ ì£¼ìš” ì‚¬ì—… ë¶€ë¬¸ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "íšŒì‚¬ì˜ í–¥í›„ ì „ë§ì€ ì–´ë–»ìŠµë‹ˆê¹Œ?"
        ]
        
        print("\nğŸ¯ ì‚¬ì—… í˜„í™© ì§ˆë¬¸ í…ŒìŠ¤íŠ¸:")
        for i, question in enumerate(test_questions, 1):
            print(f"\n[Q{i}] {question}")
            answer = rag_system.query(question)
            print(f"[A{i}] {answer}")
        
        print("\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
            
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_azure_rag_system()
