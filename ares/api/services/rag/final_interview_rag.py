from __future__ import annotations

"""
RAG Interview Bot (Detailed Final Report Edition)

# CHANGELOG
- [NCS Normalize] ëª¨ë“  ì§€ì ì—ì„œ self.ncs_context ì ‘ê·¼ ì „ì— dict ë³´ì¥(_ensure_ncs_dict)
- [Safe Access] .get ì²´ì´ë‹ ì „ì— isinstance(dict) ê°€ë“œ
- [Plan Robustness] ë””ìì¸ ì‹¤íŒ¨ ì‹œ í•­ìƒ dict ë°˜í™˜ {"interview_plan": []} í˜•íƒœ ë³´ì¥
- [Opening Hints] NCS íŒíŠ¸ êµ¬ì„±ë„ íƒ€ì… ê°€ë“œ
- [Misc] ë¡œê·¸/ì£¼ì„ ì •ë¦¬ (ê¸°ëŠ¥ ë³€í™” ì—†ìŒ)

ê¸°ë³¸ ê¸°ëŠ¥/êµ¬ì¡°ëŠ” ê¸°ì¡´ ë²„ì „ê³¼ ë™ì¼í•©ë‹ˆë‹¤. (ì„¤ê³„/ë¶„ì„/íŒ”ë¡œì—…/ë¦¬í¬íŠ¸ íŒŒì´í”„ë¼ì¸)
ì›ë³¸ ë ˆì´ì•„ì›ƒ ì°¸ê³ : :contentReference[oaicite:0]{index=0}
"""

import json
import re
import traceback
import unicodedata
from typing import Any, Dict, List, Optional

from openai import AzureOpenAI
from django.conf import settings
from unidecode import unidecode

# RAG ì‹œìŠ¤í…œ
from .new_azure_rag_llamaindex import AzureBlobRAGSystem
# ì›¹ ê²€ìƒ‰ ë„êµ¬
from .tool_code import google_search
# í”„ë¡¬í”„íŠ¸ (ê¸°ì¡´ ì„¸íŠ¸)
from ares.api.services.prompt import (
    INTERVIEWER_PERSONAS,
    DIFFICULTY_INSTRUCTIONS,
    prompt_interview_designer,
    prompt_resume_analyzer,
    prompt_rag_answer_analysis,
    prompt_rag_json_correction,
    prompt_rag_follow_up_question,
    prompt_rag_final_report,
    prompt_identifier,
    prompt_extractor,
    prompt_scorer,
    prompt_score_explainer,
    prompt_coach,
    prompt_bias_checker,
    prompt_model_answer,
    prompt_icebreaker_question,           # New
    prompt_self_introduction_question,    # New
    prompt_motivation_question,           # New
    prompt_followup_v2,                   # New
)
from ares.api.utils.ai_utils import safe_extract_json


# ============================ ë‚´ë¶€ ìƒì„¸ ë¦¬í¬íŠ¸ í”„ë¡¬í”„íŠ¸ ============================
_DETAILED_SECTION_PROMPT = """
You are a rigorous interview auditor. Return ONLY valid JSON.

[Goal]
For each Q/A below, produce a detailed dossier including:
- question_intent: why this was asked in this role/company context
- model_answer: an exemplary, structured answer (400~800 chars, framework tagged)
- user_answer_structure: framework extraction and missing elements
- scoring: main/ext scores (0~5 style, integers), rationale, red/green flags
- coaching: strengths, improvements, next steps (actionable)
- additional_followups: 3 precise follow-up questions not asked yet
- fact_checks: claim-by-claim verdicts with brief rationale
- ncs_alignment: relevant NCS titles (if any) and how they map
- risk_notes: any hiring risks signaled by the answer

[Context]
- company: {company_name}
- role: {job_title}
- persona: {persona_description}
- evaluation_focus: {evaluation_focus}
- business_info: {business_info}
- ncs_titles: {ncs_titles}

[InputItems]
{items}

[Output JSON Schema]
{
  "per_question_dossiers": [
    {
      "question_id": "1-1",
      "question": "...",
      "question_intent": "...",
      "model_answer": "...",
      "user_answer_structure": {
        "framework": "STAR|CASE|SYSTEMDESIGN|COMPETENCY|OTHER",
        "elements_present": ["..."],
        "elements_missing": ["..."]
      },
      "scoring": {
        "applied_framework": "STAR",
        "scores_main": {"clarity": 0, "depth": 0, "evidence": 0, "relevance": 0},
        "scores_ext": {"leadership": 0, "communication": 0, "metrics": 0},
        "scoring_reason": "..."
      },
      "coaching": {
        "strengths": ["..."],
        "improvements": ["..."],
        "next_steps": ["..."]
      },
      "additional_followups": ["Q1","Q2","Q3"],
      "fact_checks": [{"claim":"...","verdict":"ì§€ì›|ë¶ˆì¶©ë¶„|ë°˜ë°•","rationale":"..."}],
      "ncs_alignment": ["..."],
      "risk_notes": ["..."]
    }
  ]
}
"""

_DETAILED_OVERVIEW_PROMPT = """
You are a head interviewer producing a FINAL exhaustive interview report. Return ONLY valid JSON.

[Goal]
Merge per-question dossiers, the interview plan, resume feedback, and transcript to produce:
- overall_summary (2~4 paragraphs)
- interview_flow_rationale: why this sequence made sense; what was tested each stage
- strengths_matrix: thematic clusters with evidence refs (question_ids)
- weaknesses_matrix: same as above, with risk severity
- score_aggregation: averages, spread, calibration notes
- missed_opportunities: what strong answers were expected but missing
- potential_followups_global: 5-10 best follow-ups not yet asked
- resume_feedback (verbatim or summarized if too long)
- hiring_recommendation: "strong_hire|hire|no_hire" with explicit reasons
- next_actions: concrete steps before offer/no-offer (e.g., reference checks, take-home)
- question_by_question_feedback: per-question cards (intent/model answer/followups)

[Context]
- company: {company_name}
- role: {job_title}
- persona: {persona_description}
- final_report_goal: {final_report_goal}
- evaluation_focus: {evaluation_focus}

[Inputs]
- interview_plan: {interview_plan_json}
- resume_feedback_analysis: {resume_feedback_json}
- transcript_digest: {transcript_digest}
- per_question_dossiers: {per_question_dossiers}

[Output JSON Schema]
{
  "overall_summary": "...",
  "interview_flow_rationale": "...",
  "strengths_matrix": [{"theme":"...","evidence":["1-2","2-1"]}],
  "weaknesses_matrix": [{"theme":"...","severity":"low|medium|high","evidence":["..."]}],
  "score_aggregation": {
    "main_avg": {},
    "ext_avg": {},
    "calibration": "..."
  },
  "missed_opportunities": ["..."],
  "potential_followups_global": ["..."],
  "resume_feedback": {
    "job_fit_assessment": "...",
    "strengths_and_opportunities": "...",
    "gaps_and_improvements": "..."
  },
  "hiring_recommendation": "strong_hire|hire|no_hire",
  "next_actions": ["..."],
  "question_by_question_feedback": [
    {
      "question_id": "1-1",
      "stage": "...",
      "objective": "...",
      "question": "...",
      "question_intent": "...",
      "evaluation": {
        "applied_framework": "STAR",
        "scores_main": {},
        "scores_ext": {},
        "feedback": "..."
      },
      "model_answer": "...",
      "additional_followups": ["..."]
    }
  ]
}
"""


# ================================ ìœ í‹¸ë¦¬í‹° ================================
def _escape_special_chars(text: str) -> str:
    pattern = r'([+\-&|!(){}\[\]^"~*?:\\])'
    return re.sub(pattern, r'\\\1', text or "")


def _natural_num(s: str) -> int:
    try:
        digits = "".join(ch for ch in s if ch.isdigit())
        return int(digits) if digits else 10**6
    except Exception:
        return 10**6


def _truncate(s: str, limit: int, tail: str = "â€¦(truncated)") -> str:
    if not isinstance(s, str):
        s = str(s or "")
    return s if len(s) <= limit else (s[: max(0, limit - len(tail))] + tail)


def _extract_from_korean_schema(plan_data: Any) -> List[Dict]:
    """í•œê¸€ ìŠ¤í‚¤ë§ˆ -> í‘œì¤€ ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜: list[{stage, objective?, questions:[...]}]  :contentReference[oaicite:1]{index=1}"""
    if not isinstance(plan_data, (dict, list)):
        return []

    root = plan_data
    if isinstance(root, dict) and "ë©´ì ‘ ê³„íš" in root and isinstance(root["ë©´ì ‘ ê³„íš"], dict):
        stages_dict = root["ë©´ì ‘ ê³„íš"]
    elif isinstance(root, dict) and any(k.endswith("ë‹¨ê³„") for k in root.keys()):
        stages_dict = root
    else:
        return []

    norm: List[Dict] = []
    for stage_key in sorted(stages_dict.keys(), key=_natural_num):
        stage_block = stages_dict.get(stage_key, {})
        if not isinstance(stage_block, dict):
            continue

        objective = (stage_block.get("ëª©í‘œ") or stage_block.get("ëª© ì ") or "").strip() or None

        q_keys = ("ì§ˆë¬¸", "í•µì‹¬ ì§ˆë¬¸", "ë¬¸í•­", "questions")
        qs_raw = None
        for k in q_keys:
            if k in stage_block:
                qs_raw = stage_block.get(k)
                break
        if qs_raw is None:
            qs_raw = []

        qs_list: List[str] = []
        if isinstance(qs_raw, list):
            for item in qs_raw:
                if isinstance(item, str) and item.strip():
                    qs_list.append(item.strip())
                elif isinstance(item, dict):
                    q = (
                        item.get("ì§ˆë¬¸")
                        or item.get("question")
                        or item.get("Q")
                        or item.get("í…ìŠ¤íŠ¸")
                        or item.get("text")
                    )
                    if isinstance(q, str) and q.strip():
                        qs_list.append(q.strip())
        elif isinstance(qs_raw, dict):
            q = (
                qs_raw.get("ì§ˆë¬¸")
                or qs_raw.get("question")
                or qs_raw.get("Q")
                or qs_raw.get("í…ìŠ¤íŠ¸")
                or qs_raw.get("text")
            )
            if isinstance(q, str) and q.strip():
                qs_list.append(q.strip())

        fixed = []
        for q in qs_list:
            q = unicodedata.normalize("NFKC", q)
            if len(q) > 260:
                parts = re.split(r"(?<=[.!?])\s+", q)
                fixed.append(parts[0] if parts and parts[0] else q[:260])
            else:
                fixed.append(q)

        if fixed:
            norm.append({"stage": stage_key, "objective": objective, "questions": fixed})
    return norm


def _debug_print_raw_json(label: str, payload: str):
    try:
        head = payload[:800]
        tail = payload[-400:] if len(payload) > 1200 else ""
        print(f"\n--- {label} RAW JSON (len={len(payload)}) START ---\n{head}")
        if tail:
            print("\n... (snip) ...\n")
            print(tail)
        print(f"--- {label} RAW JSON END ---\n")
    except Exception:
        pass


def _force_json_like(raw: str) -> dict | list | None:
    """ë§ˆí¬ë‹¤ìš´/ì„¤ëª…ë¬¸ ì„ì¸ ì‘ë‹µì—ì„œ ê°€ì¥ ë°”ê¹¥ìª½ JSON ë¸”ë¡ì„ ê°•ì œë¡œ ì¶”ì¶œ."""
    if not raw:
        return None
    raw2 = re.sub(r"^```(?:json)?|```$", "", raw.strip(), flags=re.MULTILINE)
    for open_ch, close_ch in [("{", "}"), ("[", "]")]:
        start = raw2.find(open_ch)
        end = raw2.rfind(close_ch)
        if start != -1 and end != -1 and end > start:
            candidate = raw2[start : end + 1]
            try:
                return json.loads(candidate)
            except Exception:
                continue
    return None


def _normalize_plan_local(plan_data: Any) -> List[Dict]:
    """ë‹¤ì–‘í•œ ë³€í˜• ìŠ¤í‚¤ë§ˆë¥¼ í‘œì¤€ list[{stage, objective?, questions:[...]}] ë¡œ ì •ê·œí™”.  :contentReference[oaicite:2]{index=2}"""
    if not plan_data:
        return []

    if isinstance(plan_data, str):
        plan_data = safe_extract_json(plan_data, default=None) or _force_json_like(plan_data) or {}

    # 1) í•œêµ­ì–´ ìŠ¤í‚¤ë§ˆ
    ko_norm = _extract_from_korean_schema(plan_data)
    if ko_norm:
        return ko_norm

    # 2) ì¼ë°˜/ì˜ë¬¸
    candidate = (
        plan_data.get("plan")
        if isinstance(plan_data, dict) and "plan" in plan_data
        else plan_data.get("interview_plan")
        if isinstance(plan_data, dict) and "interview_plan" in plan_data
        else plan_data
    )

    if isinstance(candidate, dict):
        if "stage" in candidate and any(k in candidate for k in ("questions", "question", "items")):
            candidate = [candidate]
        else:
            candidate = [v for v in candidate.values() if isinstance(v, dict)]

    if not isinstance(candidate, list):
        return []

    norm: List[Dict] = []
    for i, st in enumerate(candidate, 1):
        if not isinstance(st, dict):
            continue
        stage = st.get("stage") or f"Stage {i}"
        objective = st.get("objective") or st.get("goal") or st.get("purpose") or st.get("objectives")
        qs = st.get("questions") or st.get("question") or st.get("items") or []
        if isinstance(qs, str):
            qs = [qs]
        qs = [q.strip() for q in qs if isinstance(q, str) and q.strip()]

        fixed_qs = []
        for q in qs:
            if len(q) > 260:
                m = re.split(r"(?<=[.!?])\s+", q)
                fixed_qs.append(m[0] if m and m[0] else q[:260])
            else:
                fixed_qs.append(q)

        if fixed_qs:
            norm.append({"stage": stage, "objective": objective, "questions": fixed_qs})
    return norm


def _chunked(iterable, size):
    buf = []
    for x in iterable:
        buf.append(x)
        if len(buf) >= size:
            yield buf
            buf = []
    if buf:
        yield buf

def ensure_min_questions(plan_list: List[Dict], min_per_stage: int = 1) -> List[Dict]:
    fixed = []
    for st in plan_list:
        if not isinstance(st, dict):
            continue
        title = st.get("stage") or "Untitled Stage"
        qs = [q for q in (st.get("questions") or []) if isinstance(q, str) and q.strip()]
        if not qs:
            qs = ["í•´ë‹¹ ë‹¨ê³„ì˜ í•µì‹¬ ì—­ëŸ‰ì„ ë“œëŸ¬ë‚¼ ìˆ˜ ìˆëŠ” ìµœê·¼ ì‚¬ë¡€ë¥¼ STARë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”."]
        fixed.append({"stage": title, "objective": st.get("objective"), "questions": qs[:max(1, min_per_stage)]})
    return fixed

# ================================ ë³¸ì²´ ================================
class RAGInterviewBot:
    """RAG + LLM ê¸°ë°˜ êµ¬ì¡°í™” ë©´ì ‘ Bot (ìƒì„¸ ë¦¬í¬íŠ¸ í™•ì¥íŒ)  :contentReference[oaicite:3]{index=3}"""

    # ----------------------------- NCS ì •ê·œí™” ìœ í‹¸ -----------------------------
    @staticmethod
    def _ensure_ncs_dict(ncs_ctx: Any) -> Dict[str, Any]:
        """
        ì…ë ¥ì´ str/None/dict ë¬´ì—‡ì´ ì˜¤ë“  í•­ìƒ dict í˜•íƒœì˜ NCS ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜.
        - strì´ë©´ JSON íŒŒì‹± ì‹œë„, ì‹¤íŒ¨ ì‹œ {"ncs": [], "ncs_query": ì›ë¬¸}
        - None/ê¸°íƒ€ íƒ€ì…ì´ë©´ ë¹ˆ dict ìŠ¤í™ ë°˜í™˜
        """
        if isinstance(ncs_ctx, dict):
            return ncs_ctx
        if isinstance(ncs_ctx, str):
            try:
                j = json.loads(ncs_ctx)
                if isinstance(j, dict):
                    return j
                return {"ncs": [], "ncs_query": ncs_ctx}
            except Exception:
                return {"ncs": [], "ncs_query": ncs_ctx}
        return {"ncs": [], "ncs_query": ""}

    def __init__(
        self,
        company_name: str,
        job_title: str,
        container_name: str,
        index_name: str,
        difficulty: str = "normal",
        interviewer_mode: str = "team_lead",
        ncs_context: Optional[dict] = None,
        jd_context: str = "",
        resume_context: str = "",
        research_context: str = "",
        *,
        sync_on_init: bool = False,
        **kwargs,
    ):
        print(f"ğŸ¤– RAG ì „ìš© ì‚¬ì—… ë¶„ì„ ë©´ì ‘ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ë©´ì ‘ê´€: {interviewer_mode})...")
        self.company_name = company_name or "ì•Œìˆ˜ì—†ìŒíšŒì‚¬"
        self.job_title = job_title or "ì•Œìˆ˜ì—†ìŒì§ë¬´"
        self.difficulty = difficulty
        self.interviewer_mode = interviewer_mode
        self.ncs_context = self._ensure_ncs_dict(ncs_context or {})
        self.jd_context = _truncate(jd_context, 4000)
        self.resume_context = _truncate(resume_context, 4000)
        self.research_context = _truncate(research_context, 4000)

        self.persona = INTERVIEWER_PERSONAS.get(self.interviewer_mode, INTERVIEWER_PERSONAS["team_lead"])

        self.endpoint = getattr(settings, "AZURE_OPENAI_ENDPOINT", None)
        self.api_key = getattr(settings, "AZURE_OPENAI_KEY", None)
        self.api_version = (
            getattr(settings, "AZURE_OPENAI_API_VERSION", None)
            or getattr(settings, "API_VERSION", None)
            or "2024-08-01-preview"
        )
        self.model = (
            getattr(settings, "AZURE_OPENAI_MODEL", None)
            or getattr(settings, "AZURE_OPENAI_DEPLOYMENT", None)
            or "gpt-4o"
        )
        if not self.endpoint or not self.api_key:
            raise ValueError("Azure OpenAI endpoint/key is not set in Django settings.")

        self.client = AzureOpenAI(
            azure_endpoint=self.endpoint,
            api_key=self.api_key,
            api_version=self.api_version,
        )

        print("\nğŸ“Š Azure ì‚¬ì—… ë¶„ì„ RAG ì‹œìŠ¤í…œ ì—°ë™...")
        self.rag_system = None
        self.rag_ready = False
        self._bizinfo_cache: Dict[str, str] = {}

        try:
            self.rag_system = AzureBlobRAGSystem(container_name=container_name, index_name=index_name)
            blobs = list(self.rag_system.container_client.list_blobs())
            if not blobs:
                print(f"âš ï¸ ê²½ê³ : Azure Blob ì»¨í…Œì´ë„ˆ '{container_name}'ì— ë¶„ì„í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return

            print(f"âœ… Azure RAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ. {len(blobs)}ê°œì˜ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.")
            if sync_on_init:
                print("ğŸ”„ Azure AI Search ì¸ë±ìŠ¤ ìë™ ë™ê¸°í™” ì‹œì‘...(sync_on_init=True)")
                self.rag_system.sync_index(company_name_filter=self.company_name)
            else:
                print("â© ì¸ë±ìŠ¤ ë™ê¸°í™” ìƒëµ(sync_on_init=False) â€” í•„ìš” ì‹œ ì™¸ë¶€ ì—”ë“œí¬ì¸íŠ¸/ê´€ë¦¬ìì—ì„œ ìˆ˜í–‰")

            self.rag_ready = True

        except Exception as e:
            print(f"âŒ RAG ì‹œìŠ¤í…œ ì—°ë™ ì‹¤íŒ¨: {e}")

    # ----------------------------- ë‚´ë¶€ LLM í˜¸ì¶œ -----------------------------
    def _chat_json(self, prompt: str, temperature: float = 0.2, max_tokens: int = 2000) -> str:
        sys_msg = {"role": "system", "content": "You must return ONLY a single valid JSON object. No markdown/code fences/commentary."}
        messages = [sys_msg, {"role": "user", "content": prompt}]
        kwargs = dict(model=self.model, messages=messages, temperature=temperature, max_tokens=max_tokens)

        try:
            kwargs["response_format"] = {"type": "json_object"}
            resp = self.client.chat.completions.create(**kwargs)
            return (resp.choices[0].message.content or "").strip()
        except Exception:
            resp = self.client.chat.completions.create(
                model=self.model,
                messages=[sys_msg, {"role": "user", "content": prompt + "\n\nReturn ONLY valid JSON."}],
                temperature=temperature,
                max_tokens=max_tokens,
            )
            return (resp.choices[0].message.content or "").strip()

    def _chat_text(self, prompt: str, temperature: float = 0.4, max_tokens: int = 300) -> str:
        resp = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            max_tokens=max_tokens,
        )
        return (resp.choices[0].message.content or "").strip()

    # ----------------------------- [ì‹ ê·œ] RAG í—¬í¼ -----------------------------
    def _get_company_business_info(self) -> str:
        if not self.rag_ready:
            return ""
        try:
            cache_key = f"{self.company_name}::{self.job_title}"
            if cache_key in self._bizinfo_cache:
                return self._bizinfo_cache[cache_key]

            safe_company_name = _escape_special_chars(self.company_name)
            safe_job_title = _escape_special_chars(self.job_title)
            query_text = f"{safe_company_name}ì˜ í•µì‹¬ ì‚¬ì—…, ìµœê·¼ ì‹¤ì , ì£¼ìš” ë¦¬ìŠ¤í¬, ê·¸ë¦¬ê³  {safe_job_title} ì§ë¬´ì™€ ê´€ë ¨ëœ íšŒì‚¬ ì •ë³´ì— ëŒ€í•´ ìš”ì•½í•´ì¤˜."
            print(f"ğŸ” '{self.rag_system.index_name}' ì¸ë±ìŠ¤ì—ì„œ íšŒì‚¬ ì •ë³´ ì¡°íšŒ: {query_text}")
            business_info_raw = self.rag_system.query(query_text)
            summary = _truncate(business_info_raw or "", 1200)
            self._bizinfo_cache[cache_key] = summary
            return summary
        except Exception as e:
            print(f"âš ï¸ íšŒì‚¬ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return ""

    # ----------------------------- í”Œëœ ìƒì„± -----------------------------
    def design_interview_plan(self) -> Dict:
        if not self.rag_ready:
            return {"error": "RAG ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "interview_plan": []}

        print(f"\nğŸ§  {self.company_name} ë§ì¶¤ ë©´ì ‘ ê³„íš ì„¤ê³„ ì¤‘ (ë‚œì´ë„: {self.difficulty}, ë©´ì ‘ê´€: {self.interviewer_mode})...")
        try:
            business_info = self._get_company_business_info()

            # NCS ìš”ì•½ ë¬¸ìì—´ (íƒ€ì… ê°€ë“œ)
            ncs_info = ""
            ncs_dict = self._ensure_ncs_dict(self.ncs_context)
            if isinstance(ncs_dict.get("ncs"), list):
                ncs_titles = [it.get("title") for it in ncs_dict["ncs"] if isinstance(it, dict) and it.get("title")]
                if ncs_titles:
                    ncs_info = f"\n\nNCS ì§ë¬´ ê´€ë ¨ ì •ë³´: {', '.join(ncs_titles[:6])}."

            persona_description = self.persona["persona_description"].replace("{company_name}", self.company_name).replace("{job_title}", self.job_title)
            difficulty_instruction = DIFFICULTY_INSTRUCTIONS.get(self.difficulty, "")

            prompt = (
                prompt_interview_designer
                .replace("{persona_description}", persona_description)
                .replace("{question_style_guide}", self.persona["question_style_guide"])
                .replace("{company_name}", self.company_name)
                .replace("{job_title}", self.job_title)
                .replace("{difficulty_instruction}", difficulty_instruction)
                .replace("{business_info}", business_info)
                .replace("{jd_context}", _truncate(self.jd_context, 1200))
                .replace("{resume_context}", _truncate(self.resume_context, 1200))
                .replace("{research_context}", _truncate(self.research_context, 1200))
                .replace("{ncs_info}", _truncate(ncs_info, 400))
            )

            raw = self._chat_json(prompt, temperature=0.3, max_tokens=3200)
            parsed = safe_extract_json(raw) or _force_json_like(raw) or {}
            normalized = _normalize_plan_local(parsed)

            # í”„ë¦¬ë£¨ë¸Œ(ì•„ì´ìŠ¤ë¸Œë ˆì´í‚¹/ìê¸°ì†Œê°œ/ë™ê¸°)
            initial_stages = [
                {
                    "stage": "ì•„ì´ìŠ¤ë¸Œë ˆì´í‚¹",
                    "objective": "ë©´ì ‘ ì‹œì‘ ì „ ê¸´ì¥ ì™„í™” ë° í¸ì•ˆí•œ ë¶„ìœ„ê¸° ì¡°ì„±",
                    "questions": [self._chat_text(prompt_icebreaker_question, temperature=0.7, max_tokens=100)]
                },
                {
                    "stage": "ìê¸°ì†Œê°œ",
                    "objective": "ì§€ì›ìì˜ ê¸°ë³¸ ì •ë³´ ë° í•µì‹¬ ì—­ëŸ‰ íŒŒì•…",
                    "questions": [self._chat_text(prompt_self_introduction_question, temperature=0.7, max_tokens=100)]
                },
                {
                    "stage": "ì§€ì› ë™ê¸°",
                    "objective": "íšŒì‚¬ ë° ì§ë¬´ì— ëŒ€í•œ ê´€ì‹¬ë„ì™€ ì´í•´ë„ í™•ì¸",
                    "questions": [self._chat_text(prompt_motivation_question, temperature=0.7, max_tokens=100)]
                },
            ]
            normalized = initial_stages + (normalized or [])

            # êµì • íŒ¨ìŠ¤
            if not normalized or all(not st.get("questions") for st in normalized):
                _debug_print_raw_json("PLAN_FIRST_PASS", raw or "")
                correction_raw = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "Return ONLY valid JSON. No markdown or commentary."},
                        {"role": "user", "content": prompt},
                        {"role": "assistant", "content": raw},
                        {"role": "user", "content": prompt_rag_json_correction},
                    ],
                    temperature=0.0,
                    max_tokens=2000,
                    response_format={"type": "json_object"},
                ).choices[0].message.content or ""
                corrected = safe_extract_json(correction_raw) or _force_json_like(correction_raw) or {}
                normalized2 = _normalize_plan_local(corrected)
                if normalized2:
                    normalized = initial_stages + normalized2
                else:
                    _debug_print_raw_json("PLAN_CORRECTION_FAILED", correction_raw)

            # í´ë°± (ë¹ˆ ê²½ìš°)
            if not normalized:
                single = self.generate_opening_question(
                    company_name=self.company_name,
                    job_title=self.job_title,
                    difficulty=self.difficulty,
                    context_hint={"business_info": business_info},
                )
                normalized = [{
                    "stage": "Opening",
                    "objective": "ì§€ì›ìì˜ ê¸°ë³¸ ì—­ëŸ‰ê³¼ ì‚¬ê³ ë°©ì‹ ê²€ì¦",
                    "questions": [single] if single else []
                }]

            print("âœ… êµ¬ì¡°í™” ë©´ì ‘ ê³„íš ìˆ˜ë¦½ ì™„ë£Œ." if any(st.get("questions") for st in normalized) else "âš ï¸ êµ¬ì¡°í™” ë©´ì ‘ ê³„íšì´ ë¹„ì–´ìˆìŒ.")
            return {"interview_plan": normalized}

        except Exception as e:
            error_msg = f"ë©´ì ‘ ê³„íš ìˆ˜ë¦½ ì‹¤íŒ¨: {e}"
            print(f"âŒ {error_msg}")
            traceback.print_exc()
            # í•­ìƒ dict í˜•íƒœ ë³´ì¥
            return {
                "error": error_msg,
                "interview_plan": [],
                "context": {
                    "ncs": [],
                    "ncs_query": self.ncs_context if isinstance(self.ncs_context, str) else "",
                    "company_info": "",
                },
            }

    # ----------------------------- ì˜¤í”„ë‹ í´ë°± -----------------------------
    def generate_opening_question(
        self,
        company_name: str,
        job_title: str,
        difficulty: str,
        context_hint: Optional[Dict] = None,
    ) -> str:
        hints = []
        if isinstance(context_hint, dict):
            bi = context_hint.get("business_info")
            if bi:
                hints.append(str(bi)[:600])
        ncs_dict = self._ensure_ncs_dict(self.ncs_context)
        ncs_titles = [it.get("title") for it in ncs_dict.get("ncs", []) if isinstance(it, dict) and it.get("title")]
        if ncs_titles:
            hints.append("NCS: " + ", ".join(ncs_titles[:5]))

        prompt = (
            f"[ì—­í• ] ë‹¹ì‹ ì€ {company_name} {job_title} ë©´ì ‘ì˜ {self.interviewer_mode} ë©´ì ‘ê´€\n"
            f"[ë‚œì´ë„] {difficulty}\n"
            "[ìš”ì²­] ì§€ì›ìì˜ ì—­ëŸ‰ì„ ê²€ì¦í•  'ì˜¤í”„ë‹ ì§ˆë¬¸' 1ë¬¸ì¥ë§Œ ì¶œë ¥.\n"
            "ëª¨í˜¸í•œ í‘œí˜„ì„ í”¼í•˜ê³ , ìˆ˜ì¹˜/ê·¼ê±°/ì‚¬ë¡€ ì œì‹œë¥¼ ìœ ë„í•  ê²ƒ.\n"
            f"[íŒíŠ¸]\n- " + ("\n- ".join(hints) if hints else "(ì—†ìŒ)")
        )
        try:
            text = self._chat_text(prompt, temperature=0.4, max_tokens=200)
            return text.strip().split("\n")[0].strip()
        except Exception as e:
            print(f"âŒ ë‹¨ê±´ ì˜¤í”„ë‹ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""

    # ----------------------------- RAG ì„œìˆ í˜• í‰ê°€ -----------------------------
    def _rag_narrative_analysis(self, question: str, answer: str) -> Dict:
        if not self.rag_ready:
            return {"error": "RAG ì‹œìŠ¤í…œ ë¯¸ì¤€ë¹„"}

        try:
            # ì›¹ ê²°ê³¼ (best effort)
            try:
                web_result = google_search.search(queries=[f"{self.company_name} {answer}"])
                if not isinstance(web_result, str):
                    web_result = _truncate(json.dumps(web_result, ensure_ascii=False), 2000)
            except Exception:
                web_result = "ê²€ìƒ‰ ì‹¤íŒ¨ ë˜ëŠ” ê²°ê³¼ ì—†ìŒ"

            safe_answer = _escape_special_chars(answer)
            internal_check_raw = self.rag_system.query(
                f"'{safe_answer}'ë¼ëŠ” ì£¼ì¥ì— ëŒ€í•œ ì‚¬ì‹¤ê´€ê³„ë¥¼ í™•ì¸í•˜ê³  ê´€ë ¨ ë°ì´í„°ë¥¼ ì°¾ì•„ì¤˜."
            )
            internal_check = _truncate(internal_check_raw or "", 1200)

            persona_desc = self.persona["persona_description"].replace("{company_name}", self.company_name).replace("{job_title}", self.job_title)
            analysis_prompt = (
                prompt_rag_answer_analysis
                .replace("{persona_description}", persona_desc)
                .replace("{evaluation_focus}", self.persona["evaluation_focus"])
                .replace("{company_name}", self.company_name)
                .replace("{question}", _truncate(question, 400))
                .replace("{answer}", _truncate(answer, 1500))
                .replace("{internal_check}", internal_check)
                .replace("{web_result}", _truncate(web_result, 1500))
            )

            raw_json = self._chat_json(analysis_prompt, temperature=0.2, max_tokens=2000)
            result = safe_extract_json(raw_json)
            if result is not None:
                return result

            # ìê°€ êµì •
            _debug_print_raw_json("RAG_FIRST_PASS", raw_json or "")
            corrected_raw = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "Return ONLY valid JSON. No markdown or commentary."},
                    {"role": "user", "content": analysis_prompt},
                    {"role": "assistant", "content": raw_json},
                    {"role": "user", "content": prompt_rag_json_correction},
                ],
                temperature=0.0,
                max_tokens=2000,
                response_format={"type": "json_object"},
            ).choices[0].message.content or ""
            final_result = safe_extract_json(corrected_raw)
            if final_result is not None:
                return final_result
            _debug_print_raw_json("RAG_CORRECTION_FAILED", corrected_raw)
            return {"error": "Failed to parse AI response after correction"}

        except Exception as e:
            print(f"âŒ RAG ì„œìˆ í˜• í‰ê°€ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return {"error": f"Failed to analyze answer (RAG): {e}"}

    # ----------------------------- êµ¬ì¡°í™” í‰ê°€ íŒŒì´í”„ë¼ì¸ -----------------------------
    def _structured_evaluation(self, role: str, answer: str) -> Dict:
        """Identifier â†’ Extractor â†’ Scorer â†’ ScoreExplainer â†’ Coach â†’ ModelAnswer â†’ BiasChecker"""
        try:
            # 1) Identifier
            id_prompt = prompt_identifier.replace("{answer}", _truncate(answer, 1800))
            id_raw = self._chat_json(id_prompt, temperature=0.1, max_tokens=800)
            id_json = safe_extract_json(id_raw) or {}
            frameworks: List[str] = id_json.get("frameworks", []) if isinstance(id_json, dict) else []
            values_summary = id_json.get("company_values_summary", "")

            # ê¸°ë³¸ í”„ë ˆì„ì›Œí¬ ì¶”ì •
            base_fw = None
            for fw in frameworks:
                if isinstance(fw, str):
                    base_fw = (fw.split("+")[0] or "").upper().strip()
                    if base_fw:
                        break
            if not base_fw:
                base_fw = "STAR"

            # 2) Extractor
            component_map = {
                "STAR": ["situation", "task", "action", "result"],
                "SYSTEMDESIGN": ["requirements", "trade_offs", "architecture", "risks"],
                "CASE": ["problem", "structure", "analysis", "recommendation"],
                "COMPETENCY": ["competency", "behavior", "impact"],
            }
            component_list = json.dumps(component_map.get(base_fw, []), ensure_ascii=False)
            extractor_prompt = (
                prompt_extractor
                .replace("{component_list}", component_list)
                .replace("{analysis_key}", "extracted")
                .replace("{framework_name}", base_fw)
                + "\n[ì§€ì›ì ë‹µë³€ ì›ë¬¸]\n"
                + _truncate(answer, 1800)
            )
            ex_raw = self._chat_json(extractor_prompt, temperature=0.2, max_tokens=1600)
            ex_json = safe_extract_json(ex_raw) or {}

            # 3) Scorer
            ncs_titles = [item.get("title") for item in self.ncs_context.get("ncs", []) if item.get("title")] if isinstance(self.ncs_context.get("ncs"), list) else []
            ncs_details = _truncate(", ".join(ncs_titles), 1200)
            persona_desc_scorer = self.persona["persona_description"].replace("{company_name}", self.company_name).replace("{job_title}", self.job_title)
            scorer_prompt = (
                prompt_scorer
                .replace("{framework_name}", base_fw)
                .replace("{retrieved_ncs_details}", ncs_details)
                .replace("{role}", role)
                .replace("{persona_description}", persona_desc_scorer)
                .replace("{evaluation_focus}", self.persona["evaluation_focus"])
                + "\n[ì§€ì›ì ë‹µë³€ ì›ë¬¸]\n"
                + _truncate(answer, 1800)
            )
            sc_raw = self._chat_json(scorer_prompt, temperature=0.2, max_tokens=1500)
            sc_json = safe_extract_json(sc_raw) or {}

            # 4) Score Explainer
            persona_desc_explainer = self.persona["persona_description"].replace("{company_name}", self.company_name).replace("{job_title}", self.job_title)
            expl_prompt = (
                prompt_score_explainer
                .replace("{framework}", json.dumps(sc_json.get("framework", base_fw), ensure_ascii=False))
                .replace("{scores_main}", json.dumps(sc_json.get("scores_main", {}), ensure_ascii=False))
                .replace("{scores_ext}", json.dumps(sc_json.get("scores_ext", {}), ensure_ascii=False))
                .replace("{scoring_reason}", _truncate(sc_json.get("scoring_reason", ""), 800))
                .replace("{role}", role)
                .replace("{persona_description}", persona_desc_explainer)
            )
            expl_raw = self._chat_json(expl_prompt, temperature=0.2, max_tokens=2000)
            expl_json = safe_extract_json(expl_raw) or {}

            # 5) Coach
            persona_desc_coach = self.persona["persona_description"].replace("{company_name}", self.company_name).replace("{job_title}", self.job_title)
            coach_prompt = (
                prompt_coach
                .replace("{persona_description}", persona_desc_coach)
                .replace("{scoring_reason}", _truncate(sc_json.get("scoring_reason", ""), 800))
                .replace("{user_answer}", _truncate(answer, 1800))
                .replace("{retrieved_ncs_details}", ncs_details)
                .replace("{role}", role)
                .replace("{company_name}", self.company_name)
            )
            coach_raw = self._chat_json(coach_prompt, temperature=0.2, max_tokens=1400)
            coach_json = safe_extract_json(coach_raw) or {}

            # 6) Model Answer
            persona_desc_model = self.persona["persona_description"].replace("{company_name}", self.company_name).replace("{job_title}", self.job_title)
            model_prompt = (
                prompt_model_answer
                .replace("{persona_description}", persona_desc_model)
                .replace("{retrieved_ncs_details}", ncs_details)
            )
            model_raw = self._chat_json(model_prompt, temperature=0.4, max_tokens=1400)
            model_json = safe_extract_json(model_raw) or {}

            # 7) Bias Checker
            def bias_sanitize(text: str) -> Dict:
                bprompt = prompt_bias_checker.replace("{any_text}", _truncate(text or "", 1600))
                braw = self._chat_json(bprompt, temperature=0.0, max_tokens=1400)
                return safe_extract_json(braw) or {}

            coach_text = json.dumps(coach_json, ensure_ascii=False)
            model_text = json.dumps(model_json, ensure_ascii=False)
            coach_bias = bias_sanitize(coach_text)
            model_bias = bias_sanitize(model_text)

            return {
                "identifier": {"frameworks": frameworks, "company_values_summary": values_summary},
                "extracted": ex_json.get("extracted") if isinstance(ex_json, dict) else ex_json,
                "scoring": sc_json,
                "calibration": expl_json,
                "coach": coach_json if not coach_bias.get("flagged") else coach_bias.get("sanitized_text", coach_json),
                "coach_bias_issues": coach_bias.get("issues", []),
                "model_answer": model_json if not model_bias.get("flagged") else model_bias.get("sanitized_text", model_json),
                "model_bias_issues": model_bias.get("issues", []),
            }
        except Exception as e:
            print(f"âŒ êµ¬ì¡°í™” í‰ê°€ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {e}")
            traceback.print_exc()
            return {"error": f"structured_evaluation_failed: {e}"}

    # ----------------------------- ê³µê°œ ë©”ì„œë“œ: ë‹µë³€ ë¶„ì„ -----------------------------
    def analyze_answer_with_rag(self, question: str, answer: str, role: Optional[str] = None) -> Dict:
        role = role or self.job_title
        print(f"    (ë‹µë³€ ë¶„ì„ ì¤‘... ë©´ì ‘ê´€: {self.interviewer_mode})")
        structured = self._structured_evaluation(role=role, answer=answer)
        rag_analysis = self._rag_narrative_analysis(question=question, answer=answer)
        return {"structured": structured, "rag_analysis": rag_analysis}

    # ----------------------------- ì¶œë ¥ í¬ë§¤í„° (CLI) -----------------------------
    def print_individual_analysis(self, analysis: Dict, question_num: str):
        if "error" in analysis:
            print(f"\nâŒ ë¶„ì„ ì˜¤ë¥˜: {analysis['error']}")
            return

        print("\n" + "=" * 70)
        print(f"ğŸ“Š [{question_num}] ë‹µë³€ ìƒì„¸ ë¶„ì„ ê²°ê³¼")
        print("=" * 70)

        # RAG Narrative
        rag = analysis.get("rag_analysis", {})
        print("\n" + "-" * 30)
        print("âœ… ì£¼ì¥ë³„ ì‚¬ì‹¤ í™•ì¸ (RAG - Fact-Checking)")
        checks = (rag or {}).get("claims_checked", [])
        if not checks:
            print("  - í™•ì¸ëœ ì£¼ì¥ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            for c in checks:
                claim = c.get("claim", "N/A")
                verdict = c.get("verdict") or "N/A"
                src = c.get("evidence_source", "")
                rationale = c.get("rationale") or "N/A"
                print(f'  - ì£¼ì¥: "{claim}"')
                print(f'    - íŒì •: {verdict} {f"({src})" if src else ""}')
                print(f'    - ê·¼ê±°: {rationale}')

        print("\n" + "-" * 30)
        print("ğŸ“ ë‚´ìš© ë¶„ì„ (RAG - Narrative)")
        summary = (rag or {}).get("analysis", "")
        print(f"  - ìš”ì•½: {summary or 'N/A'}")

        print("\n" + "-" * 30)
        print("ğŸ’¡ ì‹¤í–‰ ê°€ëŠ¥í•œ í”¼ë“œë°± (RAG - Actionable)")
        fb = (rag or {}).get("feedback", "")
        print(f"  - {fb or 'í”¼ë“œë°± ì—†ìŒ'}")

        # Structured
        st = analysis.get("structured", {})
        print("\n" + "-" * 30)
        print("ğŸ“ êµ¬ì¡°í™” ì±„ì  ìš”ì•½ (Structured Scoring)")
        sc = st.get("scoring", {})
        if sc:
            print(f"  - Framework: {sc.get('framework', 'N/A')}")
            print(f"  - Main: {json.dumps(sc.get('scores_main', {}), ensure_ascii=False)}")
            print(f"  - Ext : {json.dumps(sc.get('scores_ext', {}), ensure_ascii=False)}")
        else:
            print("  - ì±„ì  ê²°ê³¼ ì—†ìŒ")

        expl = st.get("calibration", {})
        if expl:
            tip = expl.get("overall_tip", "")
            print("  - ìº˜ë¦¬ë¸Œë ˆì´ì…˜ Tip:", tip or "N/A")

        coach = st.get("coach")
        if coach:
            print("\n  - ì½”ì¹­(ê°•ì /ê°œì„ /ì´í‰) ì œê³µë¨")

    # ----------------------------- ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± -----------------------------
    def generate_follow_up_question(
        self,
        original_question: str,
        answer: str,
        analysis: Dict,
        stage: str,
        objective: str,
        *,
        limit: int = 3,
        **kwargs,
    ) -> List[str]:
        """
        ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„±.
        - prompt_followup_v2 ì‚¬ìš©.
        - ë ˆê±°ì‹œ í˜¸ì¶œì—ì„œ limitë¥¼ kwargsë¡œ ë„˜ê²¨ë„ í—ˆìš©.
        - í•­ìƒ ë¬¸ìì—´ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜.
        """
        try:
            # ë ˆê±°ì‹œ í•˜ìœ„í˜¸í™˜: kwargsì— 'top_k'ë‚˜ 'limit'ê°€ ì˜¤ë©´ ìš°ì„  ë°˜ì˜
            if "top_k" in kwargs and isinstance(kwargs["top_k"], int):
                limit = kwargs["top_k"]
            if "limit" in kwargs and isinstance(kwargs["limit"], int):
                limit = kwargs["limit"]

            # Determine phase and question_type
            phase_map = {
                "ì•„ì´ìŠ¤ë¸Œë ˆì´í‚¹": "intro",
                "ìê¸°ì†Œê°œ": "intro",
                "ì§€ì› ë™ê¸°": "intro",
            }
            question_type_map = {
                "ì•„ì´ìŠ¤ë¸Œë ˆì´í‚¹": "icebreaking",
                "ìê¸°ì†Œê°œ": "self_intro",
                "ì§€ì› ë™ê¸°": "motivation",
            }
            current_phase = phase_map.get(stage, "core")
            current_question_type = question_type_map.get(stage, "general")  # ì½”ì–´ ë‹¨ê³„ëŠ” general

            # Prepare NCS context
            ncs_info = ""
            ncs_dict = self._ensure_ncs_dict(self.ncs_context)
            if isinstance(ncs_dict.get("ncs"), list):
                ncs_titles = [it.get("title") for it in ncs_dict["ncs"] if isinstance(it, dict) and it.get("title")]
                if ncs_titles:
                    ncs_info = f"NCS ì§ë¬´ ê´€ë ¨ ì •ë³´: {', '.join(ncs_titles[:6])}."

            persona_desc = self.persona["persona_description"].replace("{company_name}", self.company_name).replace("{job_title}", self.job_title)

            # Construct the prompt using prompt_followup_v2
            prompt = (
                prompt_followup_v2
                .replace("{persona_description}", persona_desc)
                .replace("{phase}", current_phase)
                .replace("{question_type}", current_question_type)
                .replace("{objective}", objective or "")
                .replace("{latest_answer}", _truncate(answer, 1500))
                .replace("{company_context}", self.company_name)
                .replace("{ncs}", _truncate(ncs_info, 400))
                .replace("{kpi}", "[]")  # KPI ì •ë³´ê°€ ì´ ì‹œì ì— ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸
            )

            raw = self._chat_json(prompt, temperature=0.6, max_tokens=500)
            result = safe_extract_json(raw)

            # í›„ì²˜ë¦¬ + í•˜ë“œ ì»·
            if result and isinstance(result, dict):
                followups = result.get("followups", [])
                if isinstance(followups, list):
                    clean = [fu.strip() for fu in followups if isinstance(fu, str) and fu.strip()]
                    return clean[: max(1, int(limit))] if clean else []
            return []
        except Exception as e:
            print(f"âŒ ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return []

    def get_stage_fallback_question(self, stage: str) -> str:
        mapping = {
            "ì•„ì´ìŠ¤ë¸Œë ˆì´í‚¹": "ìµœê·¼ì— ë³¸ ì‚°ì—…Â·ê¸°ìˆ  íŠ¸ë Œë“œ ì¤‘ ìš°ë¦¬ íšŒì‚¬/ì§ë¬´ì™€ ê°€ì¥ ê´€ë ¨ ê¹Šë‹¤ê³  ë³¸ ì‚¬ë¡€ë¥¼ 1ê°œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.",
            "ìê¸°ì†Œê°œ": "ìµœê·¼ 1ë…„ ë™ì•ˆ ë³¸ì¸ì´ ë‚¸ ê°€ì¥ ì¸¡ì •ê°€ëŠ¥í•œ ì„±ê³¼ í•œ ê°€ì§€ë¥¼ STARë¡œ ë§ì”€í•´ ì£¼ì„¸ìš”.",
            "ì§€ì› ë™ê¸°": "ìš°ë¦¬ íšŒì‚¬ì˜ ìµœê·¼ ì‚¬ì—…ì „ëµê³¼ ì—°ê²°í•´, í•´ë‹¹ ì§ë¬´ì—ì„œ ë³¸ì¸ì´ ì´ˆê¸° 90ì¼ ë™ì•ˆ ë‚¼ ìˆ˜ ìˆëŠ” ê°€ì‹œì  ì„±ê³¼ë¥¼ ì œì‹œí•´ ì£¼ì„¸ìš”.",
            "ê¸°ìˆ /ì§ë¬´ì—­ëŸ‰": "ìµœê·¼ ê²ªì€ ê¸°ìˆ ì  ì´ìŠˆ í•œ ê°€ì§€ë¥¼ â‘ ë¬¸ì œì •ì˜ â‘¡ê°€ì„¤ â‘¢ë¶„ì„/ì‹¤í—˜ â‘£ì˜ì‚¬ê²°ì • â‘¤ì§€í‘œë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.",
            "í”„ë¡œì íŠ¸/ë¬¸ì œí•´ê²°": "ê°€ì¥ ë³µì¡í–ˆë˜ í”„ë¡œì íŠ¸ë¥¼ ë¦¬ìŠ¤í¬/ì˜ì¡´ì„±/ìì› ì œì•½ ê´€ì ìœ¼ë¡œ ì„¤ëª…í•˜ê³ , ê²°ê³¼ì§€í‘œë¥¼ ê³µìœ í•´ ì£¼ì„¸ìš”.",
            "í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜": "ì˜ê²¬ ì¶©ëŒ ìƒí™©ì„ ì–´ë–»ê²Œ ì¡°ì •í–ˆëŠ”ì§€, í•©ì˜ê¹Œì§€ì˜ ê³¼ì •ê³¼ ì‚°ì¶œë¬¼ì„ ì•Œë ¤ ì£¼ì„¸ìš”.",
            "ë§ˆë¬´ë¦¬": "ë§ˆì§€ë§‰ìœ¼ë¡œ ê°•ì¡°í•˜ê³  ì‹¶ì€ ì¥ì  2ê°€ì§€ì™€, ì…ì‚¬ í›„ 6ê°œì›” ë¡œë“œë§µ(ë§ˆì¼ìŠ¤í†¤)ì„ ë§í•´ ì£¼ì„¸ìš”.",
        }
        return mapping.get(stage, "ìµœê·¼ ìˆ˜í–‰í•œ í•µì‹¬ ê³¼ì œë¥¼ STAR êµ¬ì¡°ë¡œ 2ë¶„ ë‚´ ìš”ì•½í•´ ì£¼ì„¸ìš”.")

    # ----------------------------- CLI ë©´ì ‘ ì‹œë‚˜ë¦¬ì˜¤ -----------------------------
    def conduct_interview(self):
        if not self.rag_ready:
            print("\nâŒ RAG ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì§€ ì•Šì•„ ë©´ì ‘ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        resume_analysis = self.analyze_resume_with_rag()
        interview_plan_data = self.design_interview_plan()
        if "error" in interview_plan_data and not interview_plan_data.get("interview_plan"):
            print(f"\nâŒ {interview_plan_data['error']}")
            return
        plan = interview_plan_data.get("interview_plan")
        if not plan:
            print("\nâŒ ë©´ì ‘ ê³„íšì„ ìˆ˜ë¦½í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return
        interview_plan = plan

        print("\n" + "=" * 70)
        print(f"ğŸ¢ {self.company_name} {self.job_title} ì§ë¬´ {self.interviewer_mode} ë©´ì ‘ì„ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.")
        print("ë©´ì ‘ì€ ë‹¨ê³„ë³„ ì§ˆë¬¸ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ /quit ì…ë ¥.")
        print("=" * 70)

        interview_transcript: List[Dict] = []
        interview_stopped = False

        for i, stage_data in enumerate(interview_plan, 1):
            stage_name = stage_data.get("stage", f"ë‹¨ê³„ {i}")
            objectives = stage_data.get("objectives") or stage_data.get("objective")
            stage_objective = objectives[0] if isinstance(objectives, list) and objectives else (objectives or "N/A")
            questions = stage_data.get("questions", [])

            print(f"\n\n--- ë©´ì ‘ ë‹¨ê³„ {i}: {stage_name} ---")
            print(f"ğŸ¯ ì´ë²ˆ ë‹¨ê³„ì˜ ëª©í‘œ: {stage_objective}")

            for q_idx, question in enumerate(questions, 1):
                question_id = f"{i}-{q_idx}"
                print(f"\n--- [ì§ˆë¬¸ {question_id}] ---")
                print(f"ğŸ‘¨â€ğŸ’¼ ë©´ì ‘ê´€: {question}")
                answer = input("ğŸ’¬ ë‹µë³€: ")

                if answer.lower() in ["/quit", "/ì¢…ë£Œ"]:
                    interview_stopped = True
                    break

                analysis = self.analyze_answer_with_rag(question, answer, role=self.job_title)

                fu_list: List[str] = []
                fu_disp = ""
                fu_answer = ""
                if analysis and "error" not in analysis:
                    fu_list = self.generate_follow_up_question(
                        original_question=question,
                        answer=answer,
                        analysis=analysis,
                        stage=stage_name,
                        objective=stage_objective,
                        limit=3
                    )
                    if fu_list:
                        fu_disp = fu_list[0]
                        print("\n--- [ê¼¬ë¦¬ ì§ˆë¬¸] ---")
                        print(f"ğŸ‘¨â€ğŸ’¼ ë©´ì ‘ê´€: {fu_disp}")
                        fu_answer = input("ğŸ’¬ ë‹µë³€: ")

                interview_transcript.append({
                    "question_id": question_id,
                    "stage": stage_name,
                    "objective": stage_objective,
                    "question": question,
                    "answer": answer,
                    "analysis": analysis,
                    "follow_up_question": fu_disp,
                    "follow_up_answer": fu_answer
                })

            if interview_stopped:
                break

        print("\nğŸ‰ ë©´ì ‘ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤.")

        if interview_transcript:
            self._generate_and_print_reports(interview_transcript, interview_plan_data, resume_analysis)

    # ----------------------------- ë¦¬í¬íŠ¸ ìƒì„±/ì¶œë ¥ -----------------------------
    def _cleanup_assessments(self, report: Dict) -> Dict:
        """assessment í•„ë“œ ê¼¬ë¦¬ ì½¤ë§ˆ ë“± ê°„ë‹¨ ì •ë¦¬."""
        try:
            comps = report.get("core_competency_analysis", [])
            for c in comps:
                a = c.get("assessment")
                if isinstance(a, str):
                    c["assessment"] = a.replace(",", "").strip()
        except Exception:
            pass
        return report

    def _generate_and_print_reports(self, transcript, plan_data, resume_analysis):
        print("\n\n" + "#" * 70)
        print(" ë©´ì ‘ ì „ì²´ ë‹µë³€ì— ëŒ€í•œ ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸")
        print("#" * 70)

        for item in transcript:
            self.print_individual_analysis(item["analysis"], item["question_id"])

        # ë©€í‹°-íŒ¨ìŠ¤ ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±
        report = self.generate_detailed_final_report(
            transcript=transcript,
            interview_plan=plan_data,
            resume_feedback_analysis=resume_analysis,
            batch_size=4,
            max_transcript_digest_chars=7000
        )
        self.print_final_report(report)

    def generate_detailed_final_report(
        self,
        transcript: List[Dict],
        interview_plan: Dict,
        resume_feedback_analysis: Dict,
        batch_size: int = 4,
        max_transcript_digest_chars: int = 6000,
    ) -> Dict:
        """ë¬¸í•­ ë°°ì¹˜ ë„ì‹œì— â†’ ì˜¤ë²„ë·° ì¢…í•© íŒ¨ìŠ¤(ì›í•˜ë˜ ìˆ˜ì¤€ì˜ ë¦¬ì¹˜ ë¦¬í¬íŠ¸)."""
        if not transcript:
            return {"error": "empty_transcript"}

        persona_desc = self.persona["persona_description"].replace("{company_name}", self.company_name).replace("{job_title}", self.job_title)
        business_info = self._get_company_business_info()
        ncs_titles = []
        if isinstance(self.ncs_context.get("ncs"), list):
            ncs_titles = [it.get("title") for it in self.ncs_context["ncs"] if it.get("title")]

        # transcript digest (follow-up í¬í•¨, ê¸¸ì´ ìƒí–¥)
        digest_lines = []
        for item in transcript:
            qid = item.get("question_id", "N/A")
            stage = item.get("stage", "N/A")
            obj = item.get("objective", "")
            rag_analysis = (item.get("analysis") or {}).get("rag_analysis", {})
            analysis_line = rag_analysis.get("analysis") or ""
            if not analysis_line:
                stc = (item.get("analysis") or {}).get("structured", {}).get("scoring", {})
                analysis_line = stc.get("scoring_reason", "")
            digest_lines.append(
                f"[{stage}] {qid} Q: {item.get('question','')}\n"
                f"  A: {_truncate(item.get('answer',''), 500)}\n"
                f"  Î£: {_truncate(analysis_line or 'ì—†ìŒ', 600)}"
            )
            if item.get("follow_up_question"):
                digest_lines.append(
                    f"  FU-Q: {item['follow_up_question']}\n"
                    f"  FU-A: {_truncate(item.get('follow_up_answer',''), 320)}"
                )
            if obj:
                digest_lines.append(f"  â–¶Objective: {obj}")
            digest_lines.append("---")
        transcript_digest = _truncate("\n".join(digest_lines), max_transcript_digest_chars)

        # per-question dossiers â€” ë°°ì¹˜ ìƒì„±
        per_question_dossiers: List[Dict] = []
        for batch in _chunked(transcript, batch_size):
            items_blob = []
            for it in batch:
                items_blob.append({
                    "question_id": it.get("question_id", ""),
                    "stage": it.get("stage", ""),
                    "objective": it.get("objective", ""),
                    "question": it.get("question", ""),
                    "user_answer": _truncate(it.get("answer", ""), 1800),
                    "analysis_hint": {
                        "structured": it.get("analysis", {}).get("structured", {}),
                        "rag": it.get("analysis", {}).get("rag_analysis", {})
                    },
                    "follow_up_asked": it.get("follow_up_question") or "",
                    "follow_up_answer": _truncate(it.get("follow_up_answer", ""), 800)
                })
            prompt = _DETAILED_SECTION_PROMPT \
                .replace("{company_name}", self.company_name) \
                .replace("{job_title}", self.job_title) \
                .replace("{persona_description}", persona_desc) \
                .replace("{evaluation_focus}", self.persona["evaluation_focus"]) \
                .replace("{business_info}", _truncate(business_info, 1000)) \
                .replace("{ncs_titles}", _truncate(", ".join(ncs_titles), 500)) \
                .replace("{items}", json.dumps(items_blob, ensure_ascii=False))

            raw = self._chat_json(prompt, temperature=0.2, max_tokens=3800)
            part = safe_extract_json(raw) or _force_json_like(raw) or {}
            part_list = part.get("per_question_dossiers", []) if isinstance(part, dict) else []
            if not isinstance(part_list, list):
                part_list = []
            per_question_dossiers.extend(part_list)

        # ì˜¤ë²„ë·° ì¢…í•©
        overview_prompt = _DETAILED_OVERVIEW_PROMPT \
            .replace("{company_name}", self.company_name) \
            .replace("{job_title}", self.job_title) \
            .replace("{persona_description}", persona_desc) \
            .replace("{final_report_goal}", self.persona["final_report_goal"]) \
            .replace("{evaluation_focus}", self.persona["evaluation_focus"]) \
            .replace("{interview_plan_json}", _truncate(json.dumps(interview_plan, ensure_ascii=False), 6000)) \
            .replace("{resume_feedback_json}", _truncate(json.dumps(resume_feedback_analysis, ensure_ascii=False), 6000)) \
            .replace("{transcript_digest}", transcript_digest) \
            .replace("{per_question_dossiers}", json.dumps(per_question_dossiers, ensure_ascii=False))

        raw_final = self._chat_json(overview_prompt, temperature=0.25, max_tokens=4000)
        final = safe_extract_json(raw_final) or _force_json_like(raw_final) or {}

        # í´ë¦°ì—…
        try:
            qitems = final.get("question_by_question_feedback", [])
            for qi in qitems:
                ev = qi.get("evaluation", {})
                if isinstance(ev, dict):
                    reason = ev.get("feedback") or ev.get("scoring_reason") or ""
                    if isinstance(reason, str):
                        ev["feedback"] = reason.strip()
        except Exception:
            pass

        return {
            "overall_summary": final.get("overall_summary", ""),
            "interview_flow_rationale": final.get("interview_flow_rationale", ""),
            "strengths_matrix": final.get("strengths_matrix", []),
            "weaknesses_matrix": final.get("weaknesses_matrix", []),
            "score_aggregation": final.get("score_aggregation", {}),
            "missed_opportunities": final.get("missed_opportunities", []),
            "potential_followups_global": final.get("potential_followups_global", []),
            "resume_feedback": final.get("resume_feedback", resume_feedback_analysis),
            "hiring_recommendation": final.get("hiring_recommendation", ""),
            "next_actions": final.get("next_actions", []),
            "question_by_question_feedback": final.get("question_by_question_feedback", []),
            # ë ˆê±°ì‹œ í˜¸í™˜ í‚¤
            "assessment_of_plan_achievement": final.get("interview_flow_rationale", ""),
            "core_competency_analysis": [],
            "growth_potential": "",
        }

    # ----------------------------- ë ˆê±°ì‹œ(ì˜µì…˜) -----------------------------
    def generate_final_report(self, transcript: List[Dict], interview_plan: Dict, resume_feedback_analysis: Dict) -> Dict:
        """ë ˆê±°ì‹œ ë‹¨ì¼ íŒ¨ìŠ¤ ë¦¬í¬íŠ¸(ìœ ì§€). ê¸´ ìš”ì•½/íŒ”ë¡œì—… í¬í•¨ + í´ë¦°ì—…."""
        print("\n\n" + "#" * 70)
        print(f" ìµœì¢… ì—­ëŸ‰ ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸(ë ˆê±°ì‹œ) ìƒì„± ì¤‘... (ë©´ì ‘ê´€: {self.interviewer_mode})")
        print("#" * 70)
        try:
            conversation_summary = ""
            for item in transcript:
                q_id = item.get("question_id", "N/A")
                rag_analysis = (item.get("analysis") or {}).get("rag_analysis", {})
                analysis_line = rag_analysis.get("analysis", "")
                if not analysis_line:
                    structured_analysis = (item.get("analysis") or {}).get("structured", {})
                    scoring_info = structured_analysis.get("scoring", {})
                    analysis_line = scoring_info.get("scoring_reason", "")

                conversation_summary += (
                    f"ì§ˆë¬¸ {q_id} ({item.get('stage', 'N/A')}): {item.get('question', '')}\n"
                    f"ë‹µë³€ {q_id}: {_truncate(item.get('answer', ''), 400)}\n"
                    f"(ê°œë³„ ë¶„ì„ ìš”ì•½: {_truncate(analysis_line or 'ë¶„ì„ ìš”ì•½ ì—†ìŒ', 600)})\n"
                )
                if item.get("follow_up_question"):
                    conversation_summary += (
                        f"(ê¼¬ë¦¬) ì§ˆë¬¸ {q_id}: {item['follow_up_question']}\n"
                        f"(ê¼¬ë¦¬) ë‹µë³€ {q_id}: {_truncate(item.get('follow_up_answer',''), 300)}\n"
                    )
                conversation_summary += "---\n"

            persona_desc = self.persona["persona_description"].replace("{company_name}", self.company_name).replace("{job_title}", self.job_title)
            report_prompt = (
                prompt_rag_final_report
                .replace("{persona_description}", persona_desc)
                .replace("{final_report_goal}", self.persona["final_report_goal"])
                .replace("{company_name}", self.company_name)
                .replace("{job_title}", self.job_title)
                .replace("{conversation_summary}", _truncate(conversation_summary, 5500))
                .replace("{interview_plan}", _truncate(json.dumps(interview_plan, ensure_ascii=False), 4200))
                .replace("{resume_feedback_analysis}", _truncate(json.dumps(resume_feedback_analysis, ensure_ascii=False), 4200))
            )

            raw = self._chat_json(report_prompt, temperature=0.3, max_tokens=4000)
            report_data = safe_extract_json(raw) or {}
            report_data = self._cleanup_assessments(report_data)
            return report_data

        except Exception as e:
            print(f"âŒ ìµœì¢… ë¦¬í¬íŠ¸(ë ˆê±°ì‹œ) ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            traceback.print_exc()
            return {"error": f"final_report_failed: {e}"}

    def print_final_report(self, report: Dict):
        if not report:
            return

        print("\n\n" + "=" * 70)
        print(f"ğŸ… {self.company_name} {self.job_title} ì§€ì›ì ìµœì¢… ì—­ëŸ‰ ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸ (ê´€ì : {self.interviewer_mode})")
        print("=" * 70)

        print("\nâ–  ë©´ì ‘ ê³„íš ë‹¬ì„±ë„/íë¦„ ê·¼ê±°\n" + "-" * 50)
        print(report.get("assessment_of_plan_achievement", report.get("interview_flow_rationale", "í‰ê°€ ì •ë³´ ì—†ìŒ.")))

        print("\nâ–  ì´í‰ (Overall Summary)\n" + "-" * 50)
        print(report.get("overall_summary", "ìš”ì•½ ì •ë³´ ì—†ìŒ."))

        strengths = report.get("strengths_matrix", [])
        weaknesses = report.get("weaknesses_matrix", [])
        if strengths:
            print("\nâ–  ê°•ì  ë§¤íŠ¸ë¦­ìŠ¤\n" + "-" * 50)
            for s in strengths:
                print(f"  - {s.get('theme','N/A')} :: evidence={s.get('evidence',[])}")
        if weaknesses:
            print("\nâ–  ì•½ì  ë§¤íŠ¸ë¦­ìŠ¤\n" + "-" * 50)
            for w in weaknesses:
                print(f"  - {w.get('theme','N/A')} (sev={w.get('severity','N/A')}) :: evidence={w.get('evidence',[])}")

        agg = report.get("score_aggregation", {})
        if agg:
            print("\nâ–  ì ìˆ˜ ì§‘ê³„/ìº˜ë¦¬ë¸Œë ˆì´ì…˜\n" + "-" * 50)
            print(json.dumps(agg, ensure_ascii=False))

        if "resume_feedback" in report:
            print("\nâ–  ì´ë ¥ì„œ í”¼ë“œë°± (Resume Feedback)\n" + "-" * 50)
            feedback = report.get("resume_feedback", {})
            if isinstance(feedback, dict):
                print(f"  - ì§ë¬´ ì í•©ì„±: {feedback.get('job_fit_assessment', 'N/A')}")
                print(f"  - ê°•ì  ë° ê¸°íšŒ: {feedback.get('strengths_and_opportunities', 'N/A')}")
                print(f"  - ê°œì„ ì : {feedback.get('gaps_and_improvements', 'N/A')}")
            else:
                print(f"  {feedback}")

        if "question_by_question_feedback" in report:
            print("\nâ–  ì§ˆë¬¸ë³„ ìƒì„¸ í”¼ë“œë°± (Question-by-Question Feedback)\n" + "-" * 50)
            for item in report.get("question_by_question_feedback", []):
                print(f"  - ì§ˆë¬¸ID: {item.get('question_id','-')} / ì§ˆë¬¸: {item.get('question', 'N/A')}")
                if item.get("stage"):
                    print(f"    - ë‹¨ê³„: {item.get('stage')}")
                if item.get("objective"):
                    print(f"    - ëª©í‘œ: {item.get('objective')}")
                print(f"    - ì§ˆë¬¸ ì˜ë„: {item.get('question_intent', 'N/A')}")
                evaluation = item.get("evaluation", {})
                if isinstance(evaluation, dict):
                    print(f"    - ì ìš© í”„ë ˆì„ì›Œí¬: {evaluation.get('applied_framework', 'N/A')}")
                    if evaluation.get("scores_main"): print(f"    - Main: {evaluation.get('scores_main')}")
                    if evaluation.get("scores_ext"): print(f"    - Ext : {evaluation.get('scores_ext')}")
                    print(f"    - í”¼ë“œë°±: {evaluation.get('feedback', 'N/A')}")
                else:
                    print(f"    - í”¼ë“œë°±: {evaluation}")
                if item.get("model_answer"):
                    print("    - ëª¨ë²”ë‹µë³€: " + _truncate(item.get("model_answer",""), 600))
                if item.get("additional_followups"):
                    print(f"    - ì¶”ê°€ ê¼¬ë¦¬ì§ˆë¬¸: {item.get('additional_followups')}")
                print("    " + "-" * 20)

        print("\n" + "=" * 70)


# ============================== CLI ì§„ì…ì  ==============================
def main():
    try:
        target_container = "interview-data"
        company_name = input("ë©´ì ‘ì„ ì§„í–‰í•  íšŒì‚¬ ì´ë¦„ (ì˜ˆ: ê¸°ì•„): ")
        safe_company_name_for_index = unidecode((company_name or '').lower()).replace(" ", "-") or "unknown"
        index_name = f"{safe_company_name_for_index}-report-index"
        job_title = input("ì§€ì› ì§ë¬´ (ì˜ˆ: ìƒì‚° - ìƒì‚°ìš´ì˜ ë° ê³µì •ê¸°ìˆ ): ")
        difficulty = input("ë©´ì ‘ ë‚œì´ë„ (easy, normal, hard): ") or "normal"
        interviewer_mode = input("ë©´ì ‘ê´€ ëª¨ë“œ (team_lead, executive): ") or "team_lead"

        print("\n" + "-" * 40)
        print(f"ëŒ€ìƒ ì»¨í…Œì´ë„ˆ: {target_container}")
        print(f"íšŒì‚¬ ì´ë¦„: {company_name}")
        print(f"AI Search ì¸ë±ìŠ¤: {index_name}")
        print(f"ë‚œì´ë„: {difficulty}")
        print(f"ë©´ì ‘ê´€ ëª¨ë“œ: {interviewer_mode}")
        print("-" * 40)

        bot = RAGInterviewBot(
            company_name=company_name,
            job_title=job_title,
            container_name=target_container,
            index_name=index_name,
            difficulty=difficulty,
            interviewer_mode=interviewer_mode,
            sync_on_init=False,  # ê¸°ë³¸ê°’: ì´ˆê¸° ë™ê¸°í™” ë¹„í™œì„±
        )
        bot.conduct_interview()

    except Exception as e:
        print(f"\nâŒ ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    main()
