# ares/api/services/resume_service.py
from __future__ import annotations
from typing import Dict, Any, List, Tuple
from dataclasses import dataclass
import json, argparse, sys
from datetime import date, datetime, timedelta
import re

from ares.api.utils.ai_utils import chat, safe_extract_json
from ares.api.utils.common_utils import get_logger, chunk_text
from ares.api.utils.search_utils import search_ncs_hybrid, format_ncs_context
from ares.api.services.ncs_service import summarize_top_ncs
from ares.api.services.company_data import get_company_description
from ares.api.services.prompts import (
    prompt_jd_preprocessor,
    prompt_jd_classifier,
    prompt_jd_keyword_extractor,
)
from ares.api.services.rag.final_interview_rag import RAGInterviewBot
from ares.api.services.ncs_data import get_ncs_categories, get_ncs_codes

_log = get_logger("resume")

__all__ = [
    "analyze_resume_or_cover",
    "compare_documents",
    "analyze_research_alignment",
    "analyze_all",
]

@dataclass
class GenConfig:
    chunk_size: int = 8000
    chunk_overlap: int = 600
    max_chunks_analyze: int = 8
    max_tokens_deep: int = 1100
    max_tokens_cmp: int = 900
    max_tokens_align: int = 900
    t_deep: float = 0.2
    t_cmp: float = 0.2
    t_align: float = 0.3
    max_docs_compare: int = 6
    max_chars_per_doc: int = 8000
    max_jd_chars: int = 8000
    max_resume_chars: int = 9000
    max_research_chars: int = 6000
    debug_log_prompts: bool = False
CFG = GenConfig()

# ---------- NCS í›„ì²˜ë¦¬ í•„í„° ë¡œì§ ----------
def ncs_post_filter(items: List[Dict], ncs_codes: Dict[str, str], top_k: int = 6) -> Tuple[List[Dict], List[Tuple[Dict, str]]]:
    if not ncs_codes:
        return items[:top_k], []

    out, rejects = [], []
    major_code = ncs_codes.get("major_code")
    middle_code = ncs_codes.get("middle_code")

    for it in items:
        item_major = str(it.get("major_code", "")).strip()
        item_middle = str(it.get("middle_code", "")).strip()

        if item_major == major_code and item_middle == middle_code:
            out.append(it)
        else:
            rejects.append((it, "ncs_code_mismatch"))
        
        if len(out) >= top_k:
            break
            
    return out, rejects

# ---------- í”„ë¡¬í”„íŠ¸(ì‹œìŠ¤í…œ) ----------
SYS_DEEP = (
    "ë„ˆëŠ” {persona}ë‹¤. ë¬¸ì„œë¥¼ JDì™€ [íšŒì‚¬ ì¸ì¬ìƒ] ê¸°ì¤€ìœ¼ë¡œ í‰ê°€Â·êµì •í•œë‹¤. "
    "ëª©í‘œ: ë§¤ì¹­ë„ í–¥ìƒ, ì •ëŸ‰ ê·¼ê±° ê°•í™”, ATS í†µê³¼ ê°€ëŠ¥ì„± ì œê³ . "
    "ì¶œë ¥ì€ í•œêµ­ì–´, ì„¹ì…˜/ë¶ˆë¦¿ ìœ„ì£¼, ì¦‰ì‹œ ë°˜ì˜ ê°€ëŠ¥í•œ êµ¬ì²´ ì˜ˆì‹œ í¬í•¨. "
    "ê¸ˆì§€ì–´: 'ì—´ì‹¬íˆ','ìµœëŒ€í•œ','ë§ì´'. ê°€ëŠ¥í•˜ë©´ ìˆ˜ì¹˜/ê¸°ê°„/ê·œëª¨/ì˜í–¥ ëª…ì‹œ.\n"
    "[íšŒì‚¬ ì¸ì¬ìƒ]\n{ideal_candidate_profile}"
)
SYS_CMP = (
    "ë„ˆëŠ” {persona}ë‹¤. ì œê³µëœ [ì‚¬ì „ ê²€ì¦ ê²°ê³¼]ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì—¬ëŸ¬ ë¬¸ì„œì˜ ì¼ê´€ì„±Â·ì •í•©ì„±ì„ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•œë‹¤. "
    "ë„ˆì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ì‚¬ì‹¤ì„ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” ê²ƒì´ì§€, ìƒˆë¡œìš´ ì‚¬ì‹¤ì„ íŒë‹¨í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë‹¤.\n"
    "[ì‚¬ì „ ê²€ì¦ ê²°ê³¼]\n{validation_summary}"
)
SYS_ALIGN = (
    "ë„ˆëŠ” {persona}ë‹¤. JD â†” ë¦¬ì„œì¹˜ ì •í•©ì„±ì„ ì ê²€í•´ ì°¨ë³„í™” í¬ì¸íŠ¸/ë¯¸ìŠ¤ë§¤ì¹˜ ë¦¬ìŠ¤í¬/ì§€ì›ì„œ ë¬¸ì¥ ì˜ˆì‹œë¥¼ ì œì‹œí•œë‹¤. í•œêµ­ì–´ ë¶ˆë¦¿."
)

class ValidationUtils:
    LANG_VALIDITY_DAYS = {
        "OPIC": 365*2, "OPIc": 365*2, "TOEIC": 365*2, 
        "TOEIC Speaking": 365*2, "TOEIC L&R": 365*2,
    }

    @staticmethod
    def parse_ymd(s: str) -> date | None:
        if not s: return None
        s = s.strip().replace(".", "-").replace("/", "-")
        m = re.match(r"^(\d{4})-(\d{1,2})(?:-(\d{1,2}))?$", s)
        if not m: return None
        y, mo, d = int(m.group(1)), int(m.group(2)), int(m.group(3) or 1)
        return date(y, mo, d)

    @staticmethod
    def check_lang_valid(exam_name: str, taken_str: str, reference: date) -> dict:
        taken = ValidationUtils.parse_ymd(taken_str)
        validity = ValidationUtils.LANG_VALIDITY_DAYS.get(exam_name, 365*2)
        if not taken:
            return {"valid": None, "reason": "ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨"}
        delta = (reference - taken).days
        if delta < 0:
            return {"valid": False, "reason": "ë¯¸ë˜ ë‚ ì§œ"}
        is_valid = delta <= validity
        return {"valid": is_valid, "reason": "ìœ íš¨" if is_valid else "ê¸°ê°„ ë§Œë£Œ"}

def _analyze_jd(jd_text: str, rag_bot: RAGInterviewBot) -> Dict[str, Any]:
    clean_prompt = prompt_jd_preprocessor.format(jd_text=jd_text)
    cleaned_jd = chat([{"role": "user", "content": clean_prompt}], temperature=0.0)

    ncs_categories = get_ncs_categories()
    classify_prompt = prompt_jd_classifier.format(jd_text=cleaned_jd, ncs_categories=json.dumps(ncs_categories, ensure_ascii=False))
    classification_result = chat([{"role": "user", "content": classify_prompt}], temperature=0.0)
    category = safe_extract_json(classification_result).get("category", "Other")
    ncs_codes = get_ncs_codes(category)

    persona = f"20ë…„ ê²½ë ¥ì˜ {category} ì „ë¬¸ í—¤ë“œí—Œí„°"
    
    business_summary = rag_bot.base.summarize_company_context(f"Summarize key business areas and strategies for {rag_bot.base.company_name}")
    
    keyword_prompt = prompt_jd_keyword_extractor.format(
        persona=persona,
        jd_text=cleaned_jd,
        business_summary=business_summary
    )
    keyword_result = chat([{"role": "user", "content": keyword_prompt}], temperature=0.1)
    keywords = safe_extract_json(keyword_result).get("keywords", [])
    
    return {"cleaned_jd": cleaned_jd, "category": category, "keywords": keywords, "business_summary": business_summary, "ncs_codes": ncs_codes, "persona": persona}

def _build_ncs_report(meta: Dict[str, Any] | None, jd_ctx: str, jd_keywords: List[str], ncs_codes: Dict[str, str], top: int = 6) -> Tuple[str, Dict]:
    try:
        job_title = ((meta or {}).get("job_title") or "").strip() or "ì§ë¬´ëª… ì—†ìŒ"
        keyword_str = " ".join(jd_keywords)
        query = f"{job_title} {keyword_str}"

        raw_hits = search_ncs_hybrid(query, top=top*4) or []
        hits, _ = ncs_post_filter(raw_hits, ncs_codes=ncs_codes, top_k=top)
        
        agg = summarize_top_ncs(query, jd_ctx, top=top) or []
        ctx_lines = format_ncs_context(hits, max_len=1000)

        structured_context = {"ncs": hits, "ncs_query": query, "jd_keywords": jd_keywords}

        if not agg and not ctx_lines: return "", structured_context

        lines = [f"## ğŸ§© NCS ìš”ì•½ (Top {top})", f"- ì§ˆì˜: `{job_title}`", f"- JD í•µì‹¬ì—­ëŸ‰: `{', '.join(jd_keywords)}`", ""]
        for i, it in enumerate(agg, 1):
            title = (it.get("ability_name") or it.get("ability_code") or f"Ability-{i}")
            lines.append(f"**{i}. {title}**")
            els = it.get("elements") or []
            if els: lines.append("  - ìš”ì†Œ: " + ", ".join(els[:5]))
            samples = it.get("criteria_samples") or []
            for s in samples[:3]: lines.append(f"  - ê¸°ì¤€: {s}")

        if ctx_lines:
            lines.append("<details><summary>NCS ì»¨í…ìŠ¤íŠ¸(ì›ë¬¸ ì¼ë¶€)</summary>\n\n" + ctx_lines + "\n</details>\n")

        return "\n".join(lines).strip(), structured_context
    except Exception as e:
        _log.warning(f"NCS ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "NCS ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", {}

def analyze_all(jd_text: str, resume_text: str, research_text: str, company_meta: Dict[str, Any]) -> Dict[str, Any]:
    rag_bot = RAGInterviewBot(company_name=company_meta.get("company_name", ""), job_title=company_meta.get("job_title", ""))
    jd_analysis = _analyze_jd(jd_text, rag_bot)
    
    persona = jd_analysis["persona"]
    
    company_name = company_meta.get("company_name", "")
    ideal_candidate_profile = get_company_description(company_name) if company_name else "ì¸ì¬ìƒ ì •ë³´ ì—†ìŒ"

    deep_out = analyze_resume_or_cover(
        resume_text, 
        jd_text=jd_analysis["cleaned_jd"], 
        meta=company_meta, 
        persona=persona,
        ideal_candidate_profile=ideal_candidate_profile
    )
    
    named_texts = {"JD": jd_analysis["cleaned_jd"], "ì´ë ¥ì„œ": resume_text}
    cmp_out = compare_documents(named_texts, meta=company_meta, resume_text_raw=resume_text, persona=persona)

    aln_out = ""
    if (research_text or "").strip():
        aln_out = analyze_research_alignment(jd_analysis["cleaned_jd"], resume_text, research_text=research_text, meta=company_meta, persona=persona)

    ncs_out, ncs_ctx = _build_ncs_report(company_meta, jd_analysis["cleaned_jd"], jd_keywords=jd_analysis["keywords"], ncs_codes=jd_analysis["ncs_codes"])

    return {"ì‹¬ì¸µë¶„ì„": deep_out, "êµì°¨ë¶„ì„": cmp_out, "ì •í•©ì„±ì ê²€": aln_out, "NCSìš”ì•½": ncs_out, "ncs_context": ncs_ctx}

def analyze_resume_or_cover(text: str, jd_text: str = "", meta: Dict[str, Any] | None = None, persona: str = "ëŒ€ê¸°ì—… ì±„ìš©ë‹´ë‹¹ì+ì»¤ë¦¬ì–´ì½”ì¹˜", ideal_candidate_profile: str = "ì¸ì¬ìƒ ì •ë³´ ì—†ìŒ") -> str:
    text = (text or "").strip()
    jd_text = (jd_text or "").strip()

    if not text and not jd_text:
        return "ì…ë ¥ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì´ë ¥ì„œ/ìì†Œì„œ ë³¸ë¬¸ ë˜ëŠ” JDë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."

    chunks: List[str] = list(chunk_text(text, CFG.chunk_size, CFG.chunk_overlap)) if text else [""]
    if len(chunks) > CFG.max_chunks_analyze:
        _log.warning(f"ì²­í¬ ìˆ˜ ì´ˆê³¼: {len(chunks)} > {CFG.max_chunks_analyze}, ìƒìœ„ë§Œ ë¶„ì„")
        chunks = chunks[:CFG.max_chunks_analyze]

    results: List[str] = []
    total = len(chunks)
    for i, ch in enumerate(chunks, 1):
        usr = f"[ë¬¸ì„œ (ë¶„í•  {i}/{total})]\n{ch}\n\n" \
              f"[ì„ íƒì  JD]\n{jd_text[:CFG.max_jd_chars]}\n\n" \
              "ìš”êµ¬ ì¶œë ¥:\n" \
              "1) í•µì‹¬ ìš”ì•½(ì§ë¬´ì—°ê´€ì„± ì¤‘ì‹¬)\n" \
              "2) JD ë° ì¸ì¬ìƒ ë§¤ì¹­ë„(ìƒ/ì¤‘/í•˜ + ê·¼ê±°: í‚¤ì›Œë“œ/ê²½í—˜/ì§€í‘œ/ì¸ì¬ìƒ ë¶€í•©)\n" \
              "3) í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€ í‘œ(ë¹ ì§„ í‚¤ì›Œë“œ í‘œì‹œ)\n" \
              "4) STAR ì‚¬ë¡€(ê° S/T/A/R-C 1~2ë¬¸ì¥ í…œí”Œë¦¿)\n" \
              "5) ì •ëŸ‰í™” ê°œì„ ì•ˆ(ì§€í‘œ/ê¸°ê°„/ê·œëª¨/ë„êµ¬: ì˜ˆë¬¸)\n" \
              "6) ATS ë¦¬ìŠ¤í¬/ìˆ˜ì •ì•ˆ(í˜•ì‹/í‚¤ì›Œë“œ/ì¤‘ë³µ/ê°€ë…ì„±)\n" \
              "7) ì²´í¬ë¦¬ìŠ¤íŠ¸(ì œì¶œ ì§ì „ ì ê²€)\n"
        
        system_prompt_content = SYS_DEEP.format(
            persona=persona,
            ideal_candidate_profile=ideal_candidate_profile
        )
        msgs = [{"role": "system", "content": system_prompt_content},
                {"role": "user", "content": usr}]
        
        out = chat(msgs, temperature=CFG.t_deep)
        if out:
            results.append(out)

    return "\n\n".join(results) if results else "í‰ê°€ ìƒì„± ì‹¤íŒ¨"

def compare_documents(named_texts: Dict[str, str], meta: Dict[str, Any] | None = None, resume_text_raw: str = "", persona: str = "ì±„ìš©ë‹´ë‹¹ì") -> str:
    validation_points = []
    # This is a placeholder for actual resume parsing logic
    # In a real scenario, you would parse `resume_text_raw` to find dates and exam names
    # For example:
    # lang_scores = parse_language_scores(resume_text_raw) -> [{"name": "OPIc", "date": "2024-09"}, ...]
    # for score in lang_scores:
    #     check = ValidationUtils.check_lang_valid(score['name'], score['date'], date.today())
    #     validation_points.append(f"- {score['name']} ({score['date']}): {check['reason']}")
    
    validation_summary = "\n".join(validation_points) or "ì‚¬ì „ ê²€ì¦ëœ íŠ¹ì´ì‚¬í•­ ì—†ìŒ."

    items = list(named_texts.items())[:CFG.max_docs_compare]
    pairs = [f"[{k}]\n{(v or '')[:CFG.max_chars_per_doc]}" for k, v in items]
    joined = "\n\n".join(pairs)

    usr = f"{joined}\n\n" \
          "ì¶œë ¥:\n" \
          "1) ì¼ê´€ì„± ë¬¸ì œ(ìˆ˜ì¹˜/ê¸°ê°„/ì—­í• /ì„±ê³¼/í‚¤ì›Œë“œ)\n" \
          "2) ëª¨ìˆœ/ëˆ„ë½(ì¦ë¹™ ë¶€ì¡±/ì‹œê³„ì—´ ì¶©ëŒ/ì±…ì„Â·ì„±ê³¼ ë¶ˆì¼ì¹˜)\n" \
          "3) ì •ë ¬ ê°€ì´ë“œ(ìš°ì„ ìˆœìœ„/í‘œí˜„ í†µì¼/ì‚­ì œÂ·ì¶”ê°€ ê¶Œê³ , ì˜ˆë¬¸)\n" \
          "4) ìµœì¢… ì ê²€í‘œ(ì²´í¬ë¦¬ìŠ¤íŠ¸)\n"
    
    msgs = [{"role": "system", "content": SYS_CMP.format(persona=persona, validation_summary=validation_summary)},
            {"role": "user", "content": usr}]

    return chat(msgs, temperature=CFG.t_cmp)

def analyze_research_alignment(
    jd_text: str,
    resume_concat: str,
    *,
    research_text: str = "",
    meta: Dict[str, Any] | None = None,
    persona: str = "ì»¤ë¦¬ì–´ì½”ì¹˜",
) -> str:
    jd_snip   = (jd_text or "")[:CFG.max_jd_chars]
    rs_snip   = (resume_concat or "")[:CFG.max_resume_chars]
    rsch_snip = (research_text or "")[:CFG.max_research_chars]

    if not jd_snip and not rs_snip and not rsch_snip:
        return "ë¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."

    body = f"[JD]\n{jd_snip}\n\n[ì§€ì›ì„œ í•©ë³¸]\n{rs_snip}\n\n"
    if rsch_snip:
        body += f"[ë¦¬ì„œì¹˜]\n{rsch_snip}\n\n"

    usr = body + (
            "ì¶œë ¥:\n"
            "1) í•µì‹¬ ì •í•©ì„±(ìš”êµ¬ì—­ëŸ‰â†”ì§€ì›ì„œ ì£¼ì¥ ì—°ê²°, KPI/ì§€í‘œ ê¸°ì¤€)\n"
            "2) ì°¨ë³„í™” í¬ì¸íŠ¸(íšŒì‚¬Â·ì§ë¬´ í¬ì§€ì…”ë‹)\n"
            "3) ë¯¸ìŠ¤ë§¤ì¹˜ ë¦¬ìŠ¤í¬(í•´ì†Œ ë°©ì•ˆ)\n"
            "4) ë¬¸ì¥ ì˜ˆì‹œ(ì§€ì›ì„œ/ìì†Œì„œìš© 2~3ë¬¸ì¥ í…œí”Œë¦¿)\n"
        )
    
    msgs = [{"role": "system", "content": SYS_ALIGN.format(persona=persona)},
            {"role": "user", "content": usr}]

    return chat(msgs, temperature=CFG.t_align)