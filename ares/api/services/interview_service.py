# ares/api/services/interview_service.py
from __future__ import annotations

from typing import List, Dict, Any, Optional
import time
import json
import argparse
import sys

from ares.api.utils.ai_utils import chat
from ares.api.utils.common_utils import get_logger
from ares.api.config import INTERVIEW_CONFIG as CFG, PROMPTS
from ares.api.utils.text_utils import (
    safe_strip,
    normalize_lines,
    dedup_preserve_order,
    too_similar,
    not_too_long,
    first_sentence,
    ensure_question_mark,
)

# ğŸ” NCS í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰/ì»¨í…ìŠ¤íŠ¸ ì£¼ì… (ì„ íƒ)
try:
    from ares.api.utils import search_utils as ncs
except ImportError:
    ncs = None

_log = get_logger("interview")

__all__ = [
    "make_outline",
    "generate_main_question_ondemand",
    "generate_followups",
    "score_answer_starc",
    "AIGenerationError",
]


class AIGenerationError(Exception):
    """AI ëª¨ë¸ ì‘ë‹µ ìƒì„±ì— ìµœì¢…ì ìœ¼ë¡œ ì‹¤íŒ¨í–ˆì„ ë•Œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸"""
    pass

# =========================
# ë‚´ë¶€ ìœ í‹¸
# =========================
def _safe_chat(
    msgs: List[Dict[str, str]],
    temperature: float,
    max_tokens: int,
    fallback: str = "",
    retries: int = 2,
    backoff: float = 0.8
) -> str:
    last_err = None
    for i in range(retries + 1):
        try:
            out = chat(msgs, temperature=temperature, max_tokens=max_tokens)
            return out or fallback
        except Exception as e:
            last_err = e
            _log.warning(f"chat() ì‹¤íŒ¨, ì¬ì‹œë„ {i}/{retries}: {e}")
            time.sleep(backoff * (2 ** i))
    
    _log.error(f"chat() ìµœì¢… ì‹¤íŒ¨: {last_err}")
    # ìµœì¢… ì‹¤íŒ¨ ì‹œ, fallback ëŒ€ì‹  ì˜ˆì™¸ ë°œìƒ
    raise AIGenerationError(f"AI ì‘ë‹µ ìƒì„±ì— ìµœì¢… ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {last_err}")

# =========================
# (ì„ íƒ) NCS ì»¨í…ìŠ¤íŠ¸ ì£¼ì…
# =========================
def _resolve_ncs_query(ncs_query: Optional[str], meta: Optional[dict]) -> str:
    q = (ncs_query or "").strip()
    if not q and meta:
        q = (meta.get("role") or meta.get("division") or meta.get("company") or "").strip()
    return q

def _build_ncs_ctx(query: Optional[str], top: int, max_len: int) -> str:
    if not ncs or not query:
        return ""
    try:
        hits = ncs.search_ncs_hybrid(query, top=top)
        ctx = ncs.format_ncs_context(hits, max_len=max_len) or ""
        _log.info(f"NCS ì»¨í…ìŠ¤íŠ¸: hits={len(hits)}, query='{query[:60]}'")
        return ctx
    except Exception as e:
        _log.warning(f"NCS ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return ""

# =========================
# ë©”íƒ€ ì£¼ì…
# =========================
def _inject_company_ctx(prompt: str, meta: dict | None) -> str:
    if not meta:
        return prompt
    
    def _s(x):
        return (x or "").strip()
    
    comp = _s(meta.get("company", ""))
    div  = _s(meta.get("division", ""))
    role = _s(meta.get("role", ""))
    loc  = _s(meta.get("location", ""))
    kpis = ", ".join([_s(x) for x in meta.get("jd_kpis",[]) if _s(x)])[:200]
    skills = ", ".join([_s(x) for x in meta.get("skills",[]) if _s(x)])[:200]
    
    ctx = (
        f"[íšŒì‚¬ ì»¨í…ìŠ¤íŠ¸]\n"
        f"- íšŒì‚¬: {comp or 'ë¯¸ìƒ'} | ë¶€ì„œ/ì§ë¬´: {div or '-'} / {role or '-'} | ê·¼ë¬´ì§€: {loc or '-'}\n"
        f"- KPI: {kpis or '-'} | ìŠ¤í‚¬: {skills or '-'}\n\n"
    )
    return ctx + prompt

# =========================
# USR ë¹Œë” (+ NCS ì»¨í…ìŠ¤íŠ¸)
# =========================
def _outline_usr(context: str, n: int, meta: dict | None, ncs_ctx: str) -> str:
    p = f"[ì»¨í…ìŠ¤íŠ¸]\n{not_too_long(context, CFG['CONTEXT_MAX_CHARS'])}\n\n"
    if ncs_ctx:
        p += f"[NCS ì»¨í…ìŠ¤íŠ¸]\n{ncs_ctx}\n\n"
    p += (
        f"ìš”êµ¬ì‚¬í•­:\n- ì„¹ì…˜ {n}ê°œ\n- í•œêµ­ì–´, ë¶ˆë¦¿ ì—†ìŒ\n- ê° ì¤„ 8~24ì, ëª…ì‚¬í˜• ìœ„ì£¼\n"
        "ì¶œë ¥: ì„¹ì…˜ëª…ë§Œ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë‚˜ì—´"
    )
    return _inject_company_ctx(p, meta)

def _main_usr(context: str, prev: List[str], difficulty: str, meta: dict | None, ncs_ctx: str) -> str:
    prev_block = "\n".join([f"- {q}" for q in (prev or [])]) or "- (ì—†ìŒ)"
    p = f"[ì»¨í…ìŠ¤íŠ¸]\n{not_too_long(context, 8000)}\n\n"
    if ncs_ctx:
        p += f"[NCS ì»¨í…ìŠ¤íŠ¸]\n{ncs_ctx}\n\n"
    p += (
        f"[ì´ë¯¸ í•œ ì§ˆë¬¸]\n{prev_block}\n\n"
        f"[ë‚œì´ë„]\n{difficulty}\n\n"
        "ì¶œë ¥: ë©”ì¸ ì§ˆë¬¸ í•œ ë¬¸ì¥ë§Œ(70ì ì´ë‚´, ëì€ ë¬¼ìŒí‘œ). ì¤‘ë³µ/ìœ ì‚¬ ê¸ˆì§€."
    )
    return _inject_company_ctx(p, meta)

def _follow_usr(main_q: str, answer: str, k: int, meta: dict | None, ncs_ctx: str) -> str:
    p = (
        f"[ë©”ì¸ ì§ˆë¬¸]\n{safe_strip(main_q)}\n\n"
        f"[ì§€ì›ì ë‹µë³€]\n{not_too_long(safe_strip(answer), CFG['ANSWER_MAX_CHARS'])}\n\n"
    )
    if ncs_ctx:
        p += f"[NCS ì»¨í…ìŠ¤íŠ¸]\n{ncs_ctx}\n\n"
    p += f"ìš”êµ¬: ê¼¬ë¦¬ì§ˆë¬¸ {k}ê°œ, ì„œë¡œ ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ì—ì„œ ìƒì„±.\nì¶œë ¥: ì¤„ë°”ê¿ˆìœ¼ë¡œ ì§ˆë¬¸ë§Œ ë‚˜ì—´"
    return _inject_company_ctx(p, meta)

def _starc_usr(q: str, a: str, meta: dict | None, ncs_ctx: str) -> str:
    p = f"[ì§ˆë¬¸]\n{safe_strip(q)}\n\n[ë‹µë³€]\n{safe_strip(a)}\n\n"
    if ncs_ctx:
        p += f"[NCS ì»¨í…ìŠ¤íŠ¸]\n{ncs_ctx}\n\n"
    p += "ì¶œë ¥: JSONë§Œ."
    return _inject_company_ctx(p, meta)

# =========================
# 1) ì„¹ì…˜ ì•„ì›ƒë¼ì¸
# =========================
def make_outline(context: str, n: int = 5, meta: dict | None = None, ncs_query: str | None = None) -> List[str]:
    ncs_query = _resolve_ncs_query(ncs_query, meta)
    ncs_ctx = _build_ncs_ctx(ncs_query, CFG['NCS_TOP_OUTLINE'], CFG['NCS_CTX_MAX_LEN'])
    
    msgs = [
        {"role": "system", "content": PROMPTS["SYS_OUTLINE"]},
        {"role": "user", "content": _outline_usr(context, n, meta, ncs_ctx)},
    ]

    try:
        out = _safe_chat(
            msgs,
            temperature=CFG['TEMPERATURE_OUTLINE'],
            max_tokens=CFG['MAX_TOKENS_OUTLINE'],
        )
        lines = dedup_preserve_order(normalize_lines(out))
        return lines[:n] if lines else ["ë¬¸ì œí•´ê²°", "í˜‘ì—…", "í’ˆì§ˆ", "ë¦¬ìŠ¤í¬", "ê³ ê°ì§‘ì°©"][:n]
    except AIGenerationError:
        return ["ë¬¸ì œí•´ê²°", "í˜‘ì—…", "í’ˆì§ˆ", "ë¦¬ìŠ¤í¬", "ê³ ê°ì§‘ì°©"][:n]

# =========================
# 2) ë©”ì¸ ì§ˆë¬¸ ìƒì„± (ì˜¨ë””ë§¨ë“œ 1ê°œ)
# =========================
def generate_main_question_ondemand(
    context: str,
    prev_questions: List[str],
    difficulty: str = "ë³´í†µ",
    meta: dict | None = None,
    ncs_query: str | None = None
) -> str:
    ncs_query = _resolve_ncs_query(ncs_query, meta)
    ncs_ctx = _build_ncs_ctx(ncs_query, CFG['NCS_TOP_MAIN'], CFG['NCS_CTX_MAX_LEN'])
    
    msgs = [
        {"role": "system", "content": PROMPTS["SYS_MAIN_Q"]},
        {"role": "user", "content": _main_usr(context, prev_questions, difficulty, meta, ncs_ctx)},
    ]

    fallback_q = "í•´ë‹¹ ì§ë¬´ ê´€ë ¨ í•µì‹¬ ê²½í—˜ì„ í•œ ê°€ì§€ ì‚¬ë¡€ë¡œ ì„¤ëª…í•´ ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?"
    try:
        out = _safe_chat(
            msgs,
            temperature=CFG['TEMPERATURE_MAIN'],
            max_tokens=CFG['MAX_TOKENS_MAIN'],
            fallback=fallback_q
        )
        q = first_sentence(out)
        if any(too_similar(q, pq) for pq in prev_questions or []):
            q = "ì´ì „ ì§ˆë¬¸ê³¼ ê²¹ì¹˜ì§€ ì•ŠëŠ” ë‹¤ë¥¸ í•µì‹¬ ê²½í—˜ì„ í•œ ê°€ì§€ ì„ íƒí•´ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?"
        return ensure_question_mark(q)
    except AIGenerationError:
        return fallback_q

# =========================
# 3) ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± (ë‹¨ìˆœí™” ë²„ì „)
# =========================
def generate_followups(
    main_q: str,
    answer: str,
    k: int = 3,
    main_index: Optional[int] = None,
    meta: Optional[dict] = None,
    ncs_query: str | None = None,
) -> List[str]:
    k = min(k, CFG['MAX_FOLLOW_K'])
    if k <= 0: return []

    ncs_query = _resolve_ncs_query(ncs_query, meta)
    ncs_ctx = _build_ncs_ctx(ncs_query, CFG['NCS_TOP_FOLLOW'], CFG['NCS_CTX_MAX_LEN'])

    msgs = [
        {"role": "system", "content": PROMPTS["SYS_FOLLOW"]},
        {"role": "user", "content": _follow_usr(main_q, answer, k, meta, ncs_ctx)},
    ]

    fallback_fus = [
        "í•µì‹¬ ì§€í‘œì™€ ê¸°ì¤€ì„ /ê¸°ê°„ì„ ìˆ˜ì¹˜ë¡œ ëª…í™•íˆ ì œì‹œí•´ ì£¼ì‹œê² ì–´ìš”?",
        "ë³¸ì¸ ê³ ìœ  ì˜ì‚¬ê²°ì •ê³¼ ì„ íƒ ê·¼ê±°ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ ì£¼ì‹œê² ì–´ìš”?",
        "ì£¼ìš” ë¦¬ìŠ¤í¬ì™€ ëŒ€ë¹„ ëŒ€ì•ˆ(í”ŒëœB/C)ì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”?"
    ]
    try:
        out = _safe_chat(
            msgs,
            temperature=CFG['TEMPERATURE_FOLLOW'],
            max_tokens=CFG['MAX_TOKENS_FOLLOW'],
        )
        lines = dedup_preserve_order(normalize_lines(out))[:k]
        if not lines: lines = fallback_fus[:k]
    except AIGenerationError:
        lines = fallback_fus[:k]

    if main_index is not None:
        return [f"{main_index}-{i+1}. {q.strip()}" for i, q in enumerate(lines)]
    return lines

# =========================
# 4) STAR-C í‰ê°€ (ê°€ì¤‘í•©/ë“±ê¸‰ í¬í•¨)
# =========================
def score_answer_starc(
    q: str,
    a: str,
    meta: dict | None = None,
    ncs_query: str | None = None
) -> Dict[str, Any]:
    ncs_query = _resolve_ncs_query(ncs_query, meta)
    ncs_ctx = _build_ncs_ctx(ncs_query, CFG['NCS_TOP_SCORE'], CFG['NCS_CTX_MAX_LEN'])
    
    msgs = [
        {"role": "system", "content": PROMPTS["SYS_STARC"]},
        {"role": "user", "content": _starc_usr(q, a, meta, ncs_ctx)},
    ]

    raw = ""
    try:
        raw = _safe_chat(
            msgs,
            temperature=CFG['TEMPERATURE_SCORE'],
            max_tokens=CFG['MAX_TOKENS_SCORE'],
        ).strip()
    except AIGenerationError as e:
        raw = f"í‰ê°€ ìƒì„± ì‹¤íŒ¨: {e}"

    result: Dict[str, Any] = {
        "scores": {}, "weighted_total": None, "grade": None,
        "comments": {}, "summary": []
    }
    try:
        data = json.loads(raw)
        result.update(data)

        if result["scores"] and result.get("weighted_total") is None:
            s = float(result["scores"].get("S", 0))
            t = float(result["scores"].get("T", 0))
            a_score = float(result["scores"].get("A", 0))
            r = float(result["scores"].get("R", 0))
            c = float(result["scores"].get("C", 0))
            weighted = s*1.0 + t*1.0 + a_score*1.2 + r*1.2 + c*0.8
            result["weighted_total"] = round(weighted, 2)

        if result.get("grade") is None and result.get("weighted_total") is not None:
            wt = result["weighted_total"]
            if wt >= 22.5: grade = "A"
            elif wt >= 18.0: grade = "B"
            elif wt >= 13.0: grade = "C"
            else: grade = "D"
            result["grade"] = grade

    except (json.JSONDecodeError, TypeError) as e:
        _log.warning(f"STAR-C JSON íŒŒì‹± ì‹¤íŒ¨: {e} | raw={raw[:800]}")
        result["summary"] = [raw or "í‰ê°€ ìƒì„± ì‹¤íŒ¨"]

    return result

# =========================
# CLI í…ŒìŠ¤íŠ¸ ì§„ì…ì  (ë¦¬íŒ©í† ë§ í›„)
# =========================
if __name__ == "__main__":
    # ... (CLI ë¡œì§ì€ í•„ìš” ì‹œ ì—¬ê¸°ì— ì—…ë°ì´íŠ¸)
    pass
