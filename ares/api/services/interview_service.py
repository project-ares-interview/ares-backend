# ares/api/services/interview_service.py
from __future__ import annotations

from typing import List, Dict, Any, Optional
import time
import json
import argparse
import sys

from ares.api.utils.ai_utils import chat
from ares.api.utils.common_utils import get_logger
from ares.api.config import INTERVIEW_CONFIG as CFG, PROMPTS
from ares.api.utils.text_utils import (
    safe_strip,
    normalize_lines,
    dedup_preserve_order,
    too_similar,
    not_too_long,
    first_sentence,
    ensure_question_mark,
)

# üîé NCS ÌïòÏù¥Î∏åÎ¶¨Îìú Í≤ÄÏÉâ/Ïª®ÌÖçÏä§Ìä∏ Ï£ºÏûÖ (ÏÑ†ÌÉù)
try:
    from ares.api.utils import search_utils as ncs
except ImportError:
    ncs = None

_log = get_logger("interview")

__all__ = [
    "make_outline",
    "generate_main_question_ondemand",
    "generate_followups",
    "score_answer_starc",
    "AIGenerationError",
]


class AIGenerationError(Exception):
    """AI Î™®Îç∏ ÏùëÎãµ ÏÉùÏÑ±Ïóê ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú Ïã§Ìå®ÌñàÏùÑ Îïå Î∞úÏÉùÌïòÎäî ÏòàÏô∏"""
    pass

# =========================
# ÎÇ¥Î∂Ä Ïú†Ìã∏
# =========================
def _safe_chat(
    msgs: List[Dict[str, str]],
    temperature: float,
    max_tokens: int,
    fallback: str = "",
    retries: int = 2,
    backoff: float = 0.8
) -> str:
    last_err = None
    for i in range(retries + 1):
        try:
            out = chat(msgs, temperature=temperature, max_tokens=max_tokens)
            return out or fallback
        except Exception as e:
            last_err = e
            _log.warning(f"chat() Ïã§Ìå®, Ïû¨ÏãúÎèÑ {i}/{retries}: {e}")
            time.sleep(backoff * (2 ** i))
    
    _log.error(f"chat() ÏµúÏ¢Ö Ïã§Ìå®: {last_err}")
    # ÏµúÏ¢Ö Ïã§Ìå® Ïãú, fallback ÎåÄÏã† ÏòàÏô∏ Î∞úÏÉù
    raise AIGenerationError(f"AI ÏùëÎãµ ÏÉùÏÑ±Ïóê ÏµúÏ¢Ö Ïã§Ìå®ÌñàÏäµÎãàÎã§: {last_err}")

# =========================
# (ÏÑ†ÌÉù) NCS Ïª®ÌÖçÏä§Ìä∏ Ï£ºÏûÖ
# =========================
def _resolve_ncs_query(ncs_query: Optional[str], meta: Optional[dict]) -> str:
    q = (ncs_query or "").strip()
    if not q and meta:
        q = (meta.get("role") or meta.get("division") or meta.get("company") or "").strip()
    return q

def _build_ncs_ctx(query: Optional[str], top: int, max_len: int) -> str:
    if not ncs or not query:
        return ""
    try:
        hits = ncs.search_ncs_hybrid(query, top=top)
        ctx = ncs.format_ncs_context(hits, max_len=max_len) or ""
        _log.info(f"NCS Ïª®ÌÖçÏä§Ìä∏: hits={len(hits)}, query='{query[:60]}'")
        return ctx
    except Exception as e:
        _log.warning(f"NCS Ïª®ÌÖçÏä§Ìä∏ ÏÉùÏÑ± Ïã§Ìå®: {e}")
        return ""

# =========================
# Î©îÌÉÄ Ï£ºÏûÖ
# =========================
def _inject_company_ctx(prompt: str, meta: dict | None) -> str:
    if not meta:
        return prompt
    
    def _s(x):
        return (x or "").strip()
    
    comp = _s(meta.get("company", ""))
    div  = _s(meta.get("division", ""))
    role = _s(meta.get("role", ""))
    loc  = _s(meta.get("location", ""))
    kpis = ", ".join([_s(x) for x in meta.get("jd_kpis",[]) if _s(x)])[:200]
    skills = ", ".join([_s(x) for x in meta.get("skills",[]) if _s(x)])[:200]
    
    ctx = (
        f"[ÌöåÏÇ¨ Ïª®ÌÖçÏä§Ìä∏]\n"
        f"- ÌöåÏÇ¨: {comp or 'ÎØ∏ÏÉÅ'} | Î∂ÄÏÑú/ÏßÅÎ¨¥: {div or '-'} / {role or '-'} | Í∑ºÎ¨¥ÏßÄ: {loc or '-'}\n"
        f"- KPI: {kpis or '-'} | Ïä§ÌÇ¨: {skills or '-'}\n\n"
    )
    return ctx + prompt

# =========================
# USR ÎπåÎçî (+ NCS Ïª®ÌÖçÏä§Ìä∏)
# =========================
def _outline_usr(context: str, n: int, meta: dict | None, ncs_ctx: str) -> str:
    p = f"[Ïª®ÌÖçÏä§Ìä∏]\n{not_too_long(context, CFG['CONTEXT_MAX_CHARS'])}\n\n"
    if ncs_ctx:
        p += f"[NCS Ïª®ÌÖçÏä§Ìä∏]\n{ncs_ctx}\n\n"
    p += (
        f"ÏöîÍµ¨ÏÇ¨Ìï≠:\n- ÏÑπÏÖò {n}Í∞ú\n- ÌïúÍµ≠Ïñ¥, Î∂àÎ¶ø ÏóÜÏùå\n- Í∞Å Ï§Ñ 8~24Ïûê, Î™ÖÏÇ¨Ìòï ÏúÑÏ£º\n"
        "Ï∂úÎ†•: ÏÑπÏÖòÎ™ÖÎßå Ï§ÑÎ∞îÍøàÏúºÎ°ú ÎÇòÏó¥"
    )
    return _inject_company_ctx(p, meta)

def _main_usr(context: str, prev: List[str], difficulty: str, meta: dict | None, ncs_ctx: str) -> str:
    prev_block = "\n".join([f"- {q}" for q in (prev or [])]) or "- (ÏóÜÏùå)"
    p = f"[Ïª®ÌÖçÏä§Ìä∏]\n{not_too_long(context, 8000)}\n\n"
    if ncs_ctx:
        p += f"[NCS Ïª®ÌÖçÏä§Ìä∏]\n{ncs_ctx}\n\n"
    p += (
        f"[Ïù¥ÎØ∏ Ìïú ÏßàÎ¨∏]\n{prev_block}\n\n"
        f"[ÎÇúÏù¥ÎèÑ]\n{difficulty}\n\n"
        "Ï∂úÎ†•: Î©îÏù∏ ÏßàÎ¨∏ Ìïú Î¨∏Ïû•Îßå(70Ïûê Ïù¥ÎÇ¥, ÎÅùÏùÄ Î¨ºÏùåÌëú). Ï§ëÎ≥µ/Ïú†ÏÇ¨ Í∏àÏßÄ."
    )
    return _inject_company_ctx(p, meta)

def _follow_usr(main_q: str, answer: str, k: int, meta: dict | None, ncs_ctx: str) -> str:
    p = (
        f"[Î©îÏù∏ ÏßàÎ¨∏]\n{safe_strip(main_q)}\n\n"
        f"[ÏßÄÏõêÏûê ÎãµÎ≥Ä]\n{not_too_long(safe_strip(answer), CFG['ANSWER_MAX_CHARS'])}\n\n"
    )
    if ncs_ctx:
        p += f"[NCS Ïª®ÌÖçÏä§Ìä∏]\n{ncs_ctx}\n\n"
    p += f"ÏöîÍµ¨: Íº¨Î¶¨ÏßàÎ¨∏ {k}Í∞ú, ÏÑúÎ°ú Îã§Î•∏ Ïπ¥ÌÖåÍ≥†Î¶¨ÏóêÏÑú ÏÉùÏÑ±.\nÏ∂úÎ†•: Ï§ÑÎ∞îÍøàÏúºÎ°ú ÏßàÎ¨∏Îßå ÎÇòÏó¥"
    return _inject_company_ctx(p, meta)

def _starc_usr(q: str, a: str, meta: dict | None, ncs_ctx: str) -> str:
    p = f"[ÏßàÎ¨∏]\n{safe_strip(q)}\n\n[ÎãµÎ≥Ä]\n{safe_strip(a)}\n\n"
    if ncs_ctx:
        p += f"[NCS Ïª®ÌÖçÏä§Ìä∏]\n{ncs_ctx}\n\n"
    p += "Ï∂úÎ†•: JSONÎßå."
    return _inject_company_ctx(p, meta)

# =========================
# 1) ÏÑπÏÖò ÏïÑÏõÉÎùºÏù∏
# =========================
def make_outline(context: str, n: int = 5, meta: dict | None = None, ncs_query: str | None = None) -> List[str]:
    ncs_query = _resolve_ncs_query(ncs_query, meta)
    ncs_ctx = _build_ncs_ctx(ncs_query, CFG['NCS_TOP_OUTLINE'], CFG['NCS_CTX_MAX_LEN'])
    
    msgs = [
        {"role": "system", "content": PROMPTS["SYS_OUTLINE"]},
        {"role": "user", "content": _outline_usr(context, n, meta, ncs_ctx)},
    ]

    try:
        out = _safe_chat(
            msgs,
            temperature=CFG['TEMPERATURE_OUTLINE'],
            max_tokens=CFG['MAX_TOKENS_OUTLINE'],
        )
        lines = dedup_preserve_order(normalize_lines(out))
        return lines[:n] if lines else ["Î¨∏Ï†úÌï¥Í≤∞", "ÌòëÏóÖ", "ÌíàÏßà", "Î¶¨Ïä§ÌÅ¨", "Í≥†Í∞ùÏßëÏ∞©"][:n]
    except AIGenerationError:
        return ["Î¨∏Ï†úÌï¥Í≤∞", "ÌòëÏóÖ", "ÌíàÏßà", "Î¶¨Ïä§ÌÅ¨", "Í≥†Í∞ùÏßëÏ∞©"][:n]

# =========================
# 2) Î©îÏù∏ ÏßàÎ¨∏ ÏÉùÏÑ± (Ïò®ÎîîÎß®Îìú 1Í∞ú)
# =========================
def generate_main_question_ondemand(
    context: str,
    prev_questions: List[str],
    difficulty: str = "Î≥¥ÌÜµ",
    meta: dict | None = None,
    ncs_query: str | None = None
) -> str:
    ncs_query = _resolve_ncs_query(ncs_query, meta)
    ncs_ctx = _build_ncs_ctx(ncs_query, CFG['NCS_TOP_MAIN'], CFG['NCS_CTX_MAX_LEN'])
    
    msgs = [
        {"role": "system", "content": PROMPTS["SYS_MAIN_Q"]},
        {"role": "user", "content": _main_usr(context, prev_questions, difficulty, meta, ncs_ctx)},
    ]

    fallback_q = "Ìï¥Îãπ ÏßÅÎ¨¥ Í¥ÄÎ†® ÌïµÏã¨ Í≤ΩÌóòÏùÑ Ìïú Í∞ÄÏßÄ ÏÇ¨Î°ÄÎ°ú ÏÑ§Î™ÖÌï¥ Ï£ºÏãúÍ≤†ÏäµÎãàÍπå?"
    try:
        out = _safe_chat(
            msgs,
            temperature=CFG['TEMPERATURE_MAIN'],
            max_tokens=CFG['MAX_TOKENS_MAIN'],
            fallback=fallback_q
        )
        q = first_sentence(out)
        if any(too_similar(q, pq) for pq in prev_questions or []):
            q = "Ïù¥Ï†Ñ ÏßàÎ¨∏Í≥º Í≤πÏπòÏßÄ ÏïäÎäî Îã§Î•∏ ÌïµÏã¨ Í≤ΩÌóòÏùÑ Ìïú Í∞ÄÏßÄ ÏÑ†ÌÉùÌï¥ Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú ÏÑ§Î™ÖÌï¥ Ï£ºÏãúÍ≤†ÏäµÎãàÍπå?"
        return ensure_question_mark(q)
    except AIGenerationError:
        return fallback_q

# =========================
# 3) Íº¨Î¶¨ÏßàÎ¨∏ ÏÉùÏÑ± (Îã®ÏàúÌôî Î≤ÑÏ†Ñ)
# =========================
def generate_followups(
    main_q: str,
    answer: str,
    k: int = 3,
    main_index: Optional[int] = None,
    meta: Optional[dict] = None,
    ncs_query: str | None = None,
) -> List[str]:
    k = min(k, CFG['MAX_FOLLOW_K'])
    if k <= 0: return []

    ncs_query = _resolve_ncs_query(ncs_query, meta)
    ncs_ctx = _build_ncs_ctx(ncs_query, CFG['NCS_TOP_FOLLOW'], CFG['NCS_CTX_MAX_LEN'])

    msgs = [
        {"role": "system", "content": PROMPTS["SYS_FOLLOW"]},
        {"role": "user", "content": _follow_usr(main_q, answer, k, meta, ncs_ctx)},
    ]

    fallback_fus = [
        "ÌïµÏã¨ ÏßÄÌëúÏôÄ Í∏∞Ï§ÄÏÑ†/Í∏∞Í∞ÑÏùÑ ÏàòÏπòÎ°ú Î™ÖÌôïÌûà Ï†úÏãúÌï¥ Ï£ºÏãúÍ≤†Ïñ¥Ïöî?",
        "Î≥∏Ïù∏ Í≥†Ïú† ÏùòÏÇ¨Í≤∞Ï†ïÍ≥º ÏÑ†ÌÉù Í∑ºÍ±∞Î•º Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú ÏÑ§Î™ÖÌï¥ Ï£ºÏãúÍ≤†Ïñ¥Ïöî?",
        "Ï£ºÏöî Î¶¨Ïä§ÌÅ¨ÏôÄ ÎåÄÎπÑ ÎåÄÏïà(ÌîåÎûúB/C)ÏùÄ Î¨¥ÏóáÏù¥ÏóàÎÇòÏöî?"
    ]
    try:
        out = _safe_chat(
            msgs,
            temperature=CFG['TEMPERATURE_FOLLOW'],
            max_tokens=CFG['MAX_TOKENS_FOLLOW'],
        )
        lines = dedup_preserve_order(normalize_lines(out))[:k]
        if not lines: lines = fallback_fus[:k]
    except AIGenerationError:
        lines = fallback_fus[:k]

    if main_index is not None:
        return [f"{main_index}-{i+1}. {q.strip()}" for i, q in enumerate(lines)]
    return lines

# =========================
# 4) STAR-C ÌèâÍ∞Ä (Í∞ÄÏ§ëÌï©/Îì±Í∏â Ìè¨Ìï®)
# =========================
def score_answer_starc(
    q: str,
    a: str,
    meta: dict | None = None,
    ncs_query: str | None = None
) -> Dict[str, Any]:
    ncs_query = _resolve_ncs_query(ncs_query, meta)
    ncs_ctx = _build_ncs_ctx(ncs_query, CFG['NCS_TOP_SCORE'], CFG['NCS_CTX_MAX_LEN'])
    
    msgs = [
        {"role": "system", "content": PROMPTS["SYS_STARC"]},
        {"role": "user", "content": _starc_usr(q, a, meta, ncs_ctx)},
    ]

    raw = ""
    try:
        raw = _safe_chat(
            msgs,
            temperature=CFG['TEMPERATURE_SCORE'],
            max_tokens=CFG['MAX_TOKENS_SCORE'],
        ).strip()
    except AIGenerationError as e:
        raw = f"ÌèâÍ∞Ä ÏÉùÏÑ± Ïã§Ìå®: {e}"

    result: Dict[str, Any] = {
        "scores": {}, "weighted_total": None, "grade": None,
        "comments": {}, "summary": []
    }
    try:
        data = json.loads(raw)
        result.update(data)

        if result["scores"] and result.get("weighted_total") is None:
            s = float(result["scores"].get("S", 0))
            t = float(result["scores"].get("T", 0))
            a_score = float(result["scores"].get("A", 0))
            r = float(result["scores"].get("R", 0))
            c = float(result["scores"].get("C", 0))
            weighted = s*1.0 + t*1.0 + a_score*1.2 + r*1.2 + c*0.8
            result["weighted_total"] = round(weighted, 2)

        if result.get("grade") is None and result.get("weighted_total") is not None:
            wt = result["weighted_total"]
            if wt >= 22.5: grade = "A"
            elif wt >= 18.0: grade = "B"
            elif wt >= 13.0: grade = "C"
            else: grade = "D"
            result["grade"] = grade

    except (json.JSONDecodeError, TypeError) as e:
        _log.warning(f"STAR-C JSON ÌååÏã± Ïã§Ìå®: {e} | raw={raw[:800]}")
        result["summary"] = [raw or "ÌèâÍ∞Ä ÏÉùÏÑ± Ïã§Ìå®"]

    return result

# =========================
# CLI ÌÖåÏä§Ìä∏ ÏßÑÏûÖÏ†ê (Î¶¨Ìå©ÌÜ†ÎßÅ ÌõÑ)
# =========================
if __name__ == "__main__":
    # ... (CLI Î°úÏßÅÏùÄ ÌïÑÏöî Ïãú Ïó¨Í∏∞Ïóê ÏóÖÎç∞Ïù¥Ìä∏)
    pass
